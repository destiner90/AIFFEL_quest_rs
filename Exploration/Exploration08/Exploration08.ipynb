{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "826f259d-5ee3-4185-9e6e-b0322b27f836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80731ccf-b9d2-4a2a-b0ed-7b1c100c611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_header(text):\n",
    "    \"\"\"헤더 출력\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"  {text}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "def print_step(num, text):\n",
    "    \"\"\"단계 출력\"\"\"\n",
    "    print(f\"\\n[단계 {num}] {text}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "\n",
    "def check_dependencies():\n",
    "    \"\"\"필수 라이브러리 확인\"\"\"\n",
    "    print_step(1, \"필수 라이브러리 확인\")\n",
    "    \n",
    "    required_packages = {\n",
    "        'torch': 'PyTorch',\n",
    "        'pandas': 'Pandas',\n",
    "        'numpy': 'NumPy',\n",
    "    }\n",
    "    \n",
    "    optional_packages = {\n",
    "        'sentencepiece': 'SentencePiece',\n",
    "        'matplotlib': 'Matplotlib',\n",
    "    }\n",
    "    \n",
    "    missing_required = []\n",
    "    missing_optional = []\n",
    "    \n",
    "    # 필수 패키지 확인\n",
    "    for package, name in required_packages.items():\n",
    "        try:\n",
    "            __import__(package)\n",
    "            print(f\"✓ {name} 설치됨\")\n",
    "        except ImportError:\n",
    "            print(f\"❌ {name} 미설치\")\n",
    "            missing_required.append(package)\n",
    "    \n",
    "    # 선택 패키지 확인\n",
    "    for package, name in optional_packages.items():\n",
    "        try:\n",
    "            __import__(package)\n",
    "            print(f\"✓ {name} 설치됨 (선택)\")\n",
    "        except ImportError:\n",
    "            print(f\"⚠ {name} 미설치 (선택)\")\n",
    "            missing_optional.append(package)\n",
    "    \n",
    "    if missing_required:\n",
    "        print(f\"\\n❌ 필수 패키지가 설치되지 않았습니다:\")\n",
    "        for pkg in missing_required:\n",
    "            print(f\"   pip install {pkg}\")\n",
    "        return False\n",
    "    \n",
    "    if missing_optional:\n",
    "        print(f\"\\n⚠ 선택 패키지가 설치되지 않았습니다:\")\n",
    "        for pkg in missing_optional:\n",
    "            print(f\"   pip install {pkg}\")\n",
    "        print(\"   (계속 진행할 수 있지만 일부 기능이 제한될 수 있습니다)\")\n",
    "    \n",
    "    print(\"\\n✓ 필수 라이브러리 모두 설치됨\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb8d9ef5-d20f-466f-8fad-fe0ae8d0da14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(force=False):\n",
    "    \"\"\"데이터셋 다운로드\"\"\"\n",
    "    print_step(2, \"ChatbotData.csv 다운로드\")\n",
    "    \n",
    "    dataset_path = Path(\"ChatbotData.csv\")\n",
    "    \n",
    "    if dataset_path.exists() and not force:\n",
    "        print(f\"✓ 데이터셋 이미 존재: {dataset_path.absolute()}\")\n",
    "        \n",
    "        # 파일 크기 확인\n",
    "        size_mb = dataset_path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  파일 크기: {size_mb:.2f} MB\")\n",
    "        \n",
    "        # 데이터 샘플 확인\n",
    "        try:\n",
    "            df = pd.read_csv(dataset_path, encoding='utf-8', nrows=5)\n",
    "            print(f\"  컬럼: {list(df.columns)}\")\n",
    "            print(f\"  샘플 개수: (파일 확인 중...)\")\n",
    "            \n",
    "            df_full = pd.read_csv(dataset_path, encoding='utf-8')\n",
    "            print(f\"  전체 행: {len(df_full)}\")\n",
    "            print(f\"\\n  샘플 데이터:\")\n",
    "            for idx, row in df.iterrows():\n",
    "                if idx < 2:\n",
    "                    q = row[df.columns[0]][:30]\n",
    "                    a = row[df.columns[1]][:30]\n",
    "                    print(f\"    Q: {q}...\")\n",
    "                    print(f\"    A: {a}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠ 파일 읽기 실패: {e}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    print(\"데이터셋 다운로드 중...\")\n",
    "    url = \"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\"\n",
    "    \n",
    "    try:\n",
    "        def progress_hook(block_num, block_size, total_size):\n",
    "            downloaded = block_num * block_size\n",
    "            percent = min(100, int(100 * downloaded / total_size))\n",
    "            print(f\"  진행률: {percent}% ({downloaded/1024/1024:.1f}MB / {total_size/1024/1024:.1f}MB)\", end='\\r')\n",
    "        \n",
    "        urlretrieve(url, dataset_path, progress_hook)\n",
    "        print(\"\\n✓ 데이터셋 다운로드 완료\")\n",
    "        \n",
    "        # 파일 검증\n",
    "        if dataset_path.exists():\n",
    "            size_mb = dataset_path.stat().st_size / (1024 * 1024)\n",
    "            print(f\"✓ 파일 저장: {dataset_path.absolute()}\")\n",
    "            print(f\"✓ 파일 크기: {size_mb:.2f} MB\")\n",
    "            \n",
    "            # 데이터 확인\n",
    "            try:\n",
    "                df = pd.read_csv(dataset_path, encoding='utf-8')\n",
    "                print(f\"✓ 데이터 행: {len(df)}\")\n",
    "                print(f\"✓ 컬럼: {list(df.columns)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 데이터 읽기 실패: {e}\")\n",
    "                return False\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ 다운로드 실패\")\n",
    "            return False\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ 다운로드 중 오류: {e}\")\n",
    "        print(\"\\n수동 다운로드 방법:\")\n",
    "        print(\"1. 다음 URL로 이동:\")\n",
    "        print(f\"   {url}\")\n",
    "        print(\"2. 파일 다운로드\")\n",
    "        print(\"3. 현재 디렉토리에 저장:\")\n",
    "        print(f\"   {dataset_path.absolute()}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2653edd-8a5b-4992-922c-d2000bfdb5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_directories():\n",
    "    \"\"\"디렉토리 설정\"\"\"\n",
    "    print_step(3, \"디렉토리 설정\")\n",
    "    \n",
    "    checkpoints_dir = Path(\"checkpoints\")\n",
    "    checkpoints_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"✓ checkpoints 디렉토리: {checkpoints_dir.absolute()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d89b4a04-ab49-4d6a-ae18-a9a9def5a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config_file():\n",
    "    \"\"\"설정 파일 생성 (선택)\"\"\"\n",
    "    print_step(4, \"설정 파일 생성\")\n",
    "    \n",
    "    config_suggestions = {\n",
    "        '1': ('CPU 최적화', {\n",
    "            'D_MODEL': 256,\n",
    "            'NUM_LAYERS': 2,\n",
    "            'UNITS': 1024,\n",
    "            'BATCH_SIZE': 16,\n",
    "            'NUM_EPOCHS': 20\n",
    "        }),\n",
    "        '2': ('GPU 표준', {\n",
    "            'D_MODEL': 512,\n",
    "            'NUM_LAYERS': 4,\n",
    "            'UNITS': 2048,\n",
    "            'BATCH_SIZE': 32,\n",
    "            'NUM_EPOCHS': 40\n",
    "        }),\n",
    "        '3': ('GPU 고성능', {\n",
    "            'D_MODEL': 768,\n",
    "            'NUM_LAYERS': 6,\n",
    "            'UNITS': 4096,\n",
    "            'BATCH_SIZE': 64,\n",
    "            'NUM_EPOCHS': 60\n",
    "        }),\n",
    "    }\n",
    "    \n",
    "    print(\"권장 설정:\")\n",
    "    for key, (name, config) in config_suggestions.items():\n",
    "        print(f\"  {key}. {name}\")\n",
    "        print(f\"     D_MODEL={config['D_MODEL']}, \"\n",
    "              f\"BATCH_SIZE={config['BATCH_SIZE']}, \"\n",
    "              f\"EPOCHS={config['NUM_EPOCHS']}\")\n",
    "    \n",
    "    print(\"\\n→ Config 클래스를 직접 수정하거나\")\n",
    "    print(\"→ korean_chatbot_complete.py의 설정값을 변경하세요\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27f7b428-fb39-4b1e-b066-0af46d1251c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_requirements():\n",
    "    \"\"\"시스템 요구사항 표시\"\"\"\n",
    "    print_step(5, \"시스템 요구사항\")\n",
    "    \n",
    "    print(\"\"\"\n",
    "권장 사양:\n",
    "├─ Python: 3.8 이상\n",
    "├─ RAM: 8GB 이상\n",
    "├─ 저장공간: 1GB 이상\n",
    "└─ GPU (선택): NVIDIA CUDA 11.0+\n",
    "\n",
    "GPU 없을 경우:\n",
    "- CPU에서 작동 (느림)\n",
    "- D_MODEL=256, NUM_LAYERS=2로 설정\n",
    "- BATCH_SIZE=8로 설정\n",
    "- 학습 시간: 24~48시간\n",
    "\n",
    "GPU 있을 경우:\n",
    "- D_MODEL=512, NUM_LAYERS=4로 설정\n",
    "- BATCH_SIZE=32 이상으로 설정\n",
    "- 학습 시간: 2~4시간\n",
    "\n",
    "현재 시스템:\n",
    "\"\"\")\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"  Python: {sys.version.split()[0]}\")\n",
    "        print(f\"  PyTorch: {torch.__version__}\")\n",
    "        print(f\"  CUDA: {'사용 가능' if torch.cuda.is_available() else '사용 불가'}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"  GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "    except Exception as e:\n",
    "        print(f\"  정보 조회 실패: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ced472f8-58ec-46c2-ba9c-3be05ba63f1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "  한국어 Transformer 챗봇 - 빠른 시작 설정\n",
      "======================================================================\n",
      "\n",
      "이 스크립트는 다음을 수행합니다:\n",
      "1. 필수 라이브러리 확인\n",
      "2. ChatbotData.csv 다운로드\n",
      "3. 디렉토리 설정\n",
      "4. 시스템 정보 표시\n",
      "5. 다음 단계 안내\n",
      "\n",
      "\n",
      "[단계 1] 필수 라이브러리 확인\n",
      "----------------------------------------------------------------------\n",
      "✓ PyTorch 설치됨\n",
      "✓ Pandas 설치됨\n",
      "✓ NumPy 설치됨\n",
      "✓ SentencePiece 설치됨 (선택)\n",
      "✓ Matplotlib 설치됨 (선택)\n",
      "\n",
      "✓ 필수 라이브러리 모두 설치됨\n",
      "\n",
      "[단계 2] ChatbotData.csv 다운로드\n",
      "----------------------------------------------------------------------\n",
      "데이터셋 다운로드 중...\n",
      "  진행률: 100% (0.9MB / 0.8MB)\n",
      "✓ 데이터셋 다운로드 완료\n",
      "✓ 파일 저장: /home/jovyan/work/ChatbotData.csv\n",
      "✓ 파일 크기: 0.85 MB\n",
      "✓ 데이터 행: 11823\n",
      "✓ 컬럼: ['Q', 'A', 'label']\n",
      "\n",
      "[단계 3] 디렉토리 설정\n",
      "----------------------------------------------------------------------\n",
      "✓ checkpoints 디렉토리: /home/jovyan/work/checkpoints\n",
      "\n",
      "[단계 4] 설정 파일 생성\n",
      "----------------------------------------------------------------------\n",
      "권장 설정:\n",
      "  1. CPU 최적화\n",
      "     D_MODEL=256, BATCH_SIZE=16, EPOCHS=20\n",
      "  2. GPU 표준\n",
      "     D_MODEL=512, BATCH_SIZE=32, EPOCHS=40\n",
      "  3. GPU 고성능\n",
      "     D_MODEL=768, BATCH_SIZE=64, EPOCHS=60\n",
      "\n",
      "→ Config 클래스를 직접 수정하거나\n",
      "→ korean_chatbot_complete.py의 설정값을 변경하세요\n",
      "\n",
      "[단계 5] 시스템 요구사항\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "권장 사양:\n",
      "├─ Python: 3.8 이상\n",
      "├─ RAM: 8GB 이상\n",
      "├─ 저장공간: 1GB 이상\n",
      "└─ GPU (선택): NVIDIA CUDA 11.0+\n",
      "\n",
      "GPU 없을 경우:\n",
      "- CPU에서 작동 (느림)\n",
      "- D_MODEL=256, NUM_LAYERS=2로 설정\n",
      "- BATCH_SIZE=8로 설정\n",
      "- 학습 시간: 24~48시간\n",
      "\n",
      "GPU 있을 경우:\n",
      "- D_MODEL=512, NUM_LAYERS=4로 설정\n",
      "- BATCH_SIZE=32 이상으로 설정\n",
      "- 학습 시간: 2~4시간\n",
      "\n",
      "현재 시스템:\n",
      "\n",
      "  Python: 3.12.11\n",
      "  PyTorch: 2.7.1+cu118\n",
      "  CUDA: 사용 가능\n",
      "  GPU: Tesla T4\n",
      "  GPU 메모리: 15.7GB\n",
      "\n",
      "[단계 6] 다음 단계\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "준비가 완료되었습니다!\n",
      "\n",
      "1. korean_chatbot_complete.py 실행:\n",
      "   python korean_chatbot_complete.py\n",
      "\n",
      "2. 학습 진행 확인:\n",
      "   - 에폭마다 진행 상황 출력\n",
      "   - checkpoints/best_model.pt에 모델 저장\n",
      "   - config.json에 설정 저장\n",
      "\n",
      "3. 학습 완료 후:\n",
      "   - 자동으로 응답 생성 테스트 실행\n",
      "   - 몇 가지 샘플 입력에 대한 응답 생성\n",
      "\n",
      "4. 커스터마이징:\n",
      "   - USAGE_GUIDE_KO.md 참고\n",
      "   - Config 클래스 수정하여 설정 변경\n",
      "\n",
      "설정 변경 (선택):\n",
      "\n",
      "\n",
      "  ❶ 학습 데이터 변경:\n",
      "     Config.DATA_PATH = \"my_data.csv\"\n",
      "\n",
      "  ❷ 모델 크기 변경:\n",
      "     Config.D_MODEL = 256  (CPU) 또는 512 (GPU)\n",
      "     Config.NUM_LAYERS = 2 (빠름) 또는 4 (정확)\n",
      "\n",
      "  ❸ 학습 시간 변경:\n",
      "     Config.NUM_EPOCHS = 20 (빠름) 또는 60 (정확)\n",
      "     Config.BATCH_SIZE = 16 (메모리 절약) 또는 64 (빠름)\n",
      "\n",
      "  ❹ GPU 사용:\n",
      "     Config.USE_MIXED_PRECISION = True  (활성화)\n",
      "     Config.DEVICE = 'cuda' (자동 감지)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "  설정 완료!\n",
      "======================================================================\n",
      "\n",
      "이제 다음 명령으로 학습을 시작하세요:\n",
      "\n",
      "   python korean_chatbot_complete.py\n",
      "\n",
      "더 자세한 사용법은 USAGE_GUIDE_KO.md를 참고하세요.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"메인 함수\"\"\"\n",
    "    print_header(\"한국어 Transformer 챗봇 - 빠른 시작 설정\")\n",
    "    \n",
    "    print(\"\"\"\n",
    "이 스크립트는 다음을 수행합니다:\n",
    "1. 필수 라이브러리 확인\n",
    "2. ChatbotData.csv 다운로드\n",
    "3. 디렉토리 설정\n",
    "4. 시스템 정보 표시\n",
    "5. 다음 단계 안내\n",
    "\"\"\")\n",
    "    \n",
    "    # 1. 라이브러리 확인\n",
    "    if not check_dependencies():\n",
    "        print(\"\\n❌ 필수 라이브러리를 설치하세요:\")\n",
    "        print(\"   pip install torch pandas numpy\")\n",
    "        print(\"   pip install sentencepiece matplotlib\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # 2. 데이터 다운로드\n",
    "    if not download_dataset():\n",
    "        print(\"\\n⚠ 데이터셋을 수동으로 다운로드 후 진행하세요.\")\n",
    "        response = input(\"계속 진행하시겠습니까? (y/n): \")\n",
    "        if response.lower() != 'y':\n",
    "            sys.exit(1)\n",
    "    \n",
    "    # 3. 디렉토리 설정\n",
    "    setup_directories()\n",
    "    \n",
    "    # 4. 설정 제안\n",
    "    create_config_file()\n",
    "    \n",
    "    # 5. 요구사항 표시\n",
    "    show_requirements()\n",
    "    \n",
    "    # 6. 다음 단계\n",
    "    show_next_steps()\n",
    "    \n",
    "    print_header(\"설정 완료!\")\n",
    "    \n",
    "    print(\"\"\"\n",
    "이제 다음 명령으로 학습을 시작하세요:\n",
    "\n",
    "   python korean_chatbot_complete.py\n",
    "\n",
    "더 자세한 사용법은 USAGE_GUIDE_KO.md를 참고하세요.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n설정이 중단되었습니다.\")\n",
    "        sys.exit(0)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ 오류 발생: {e}\")\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65f88b0c-2592-4518-9d4a-fa1b01c99299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import math\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "try:\n",
    "    import sentencepiece as spm\n",
    "except ImportError:\n",
    "    print(\"sentencepiece 설치 필요: pip install sentencepiece\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ac13215-35b9-4f4a-b05b-39fa95af72fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1. 설정 클래스\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"모든 설정을 관리하는 클래스\"\"\"\n",
    "    \n",
    "    # 프로젝트 설정\n",
    "    PROJECT_NAME = \"korean_transformer_chatbot_v2\"\n",
    "    SAVE_DIR = Path(\"checkpoints\")\n",
    "    SAVE_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    # ========== 데이터 설정 ==========\n",
    "    DATA_PATH = \"korean_qa_data.csv\"  # 다운로드 받은 파일 경로\n",
    "    VOCAB_SIZE_SPM = 8000          # SentencePiece 어휘 크기\n",
    "    MAX_SEQ_LENGTH = 50            # 최대 시퀀스 길이\n",
    "    MAX_SAMPLES = None             # None = 전체, 정수 = 제한\n",
    "    TRAIN_VALID_SPLIT = 0.9        # 학습:검증 비율\n",
    "    \n",
    "    # ========== 모델 아키텍처 (개선됨) ==========\n",
    "    D_MODEL = 768                  # 임베딩 차원\n",
    "    NUM_LAYERS = 6                 # 인코더/디코더 레이어\n",
    "    NUM_HEADS = 8                  # 멀티헤드 어텐션 헤드 수\n",
    "    UNITS = 3072                   # 피드포워드 네트워크 차원\n",
    "    DROPOUT = 0.3                  # 드롭아웃 비율\n",
    "    \n",
    "    # ========== 학습 설정 ==========\n",
    "    BATCH_SIZE = 16                # 배치 크기 (GPU 메모리 고려)\n",
    "    NUM_EPOCHS = 100                # 에폭 수\n",
    "    LEARNING_RATE = 0.0001         # 기본 학습률\n",
    "    WARMUP_STEPS = 8000            # 워밍업 스텝\n",
    "    ADAM_BETAS = (0.9, 0.98)       # Adam 베타값\n",
    "    WEIGHT_DECAY = 1e-4            # L2 정규화\n",
    "    GRADIENT_CLIP = 0.5            # 그래디언트 클리핑\n",
    "    LABEL_SMOOTHING = 0.2          # 라벨 스무딩\n",
    "    \n",
    "    # ========== 정규화 및 최적화 ==========\n",
    "    USE_MIXED_PRECISION = True     # 혼합 정밀도 학습\n",
    "    EARLY_STOPPING_PATIENCE = 15    # 조기 종료 인내도\n",
    "    SAVE_INTERVAL = 500            # 체크포인트 저장 간격\n",
    "    LOG_INTERVAL = 100             # 로그 출력 간격\n",
    "    \n",
    "    # ========== 추론 설정 ==========\n",
    "    BEAM_SIZE = 5                  # Beam search 빔 크기\n",
    "    LENGTH_PENALTY = 0.6           # 길이 패널티\n",
    "    TEMPERATURE = 0.8              # 온도\n",
    "    TOP_K = 40                     # Top-k 샘플링\n",
    "    TOP_P = 0.9                    # Nucleus 샘플링\n",
    "    \n",
    "    # ========== 기기 설정 ==========\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    SEED = 42\n",
    "    \n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"설정을 JSON 직렬화 가능한 딕셔너리로 변환\"\"\"\n",
    "        config_dict = {}\n",
    "        for k, v in cls.__dict__.items():\n",
    "            if not k.startswith('_') and k.isupper():\n",
    "                # Path 객체를 문자열로 변환\n",
    "                if isinstance(v, Path):\n",
    "                    config_dict[k] = str(v)\n",
    "                # torch.device 객체를 문자열로 변환\n",
    "                elif hasattr(torch, 'device') and isinstance(v, torch.device):\n",
    "                    config_dict[k] = str(v)\n",
    "                # 튜플은 리스트로 변환 (JSON 호환성)\n",
    "                elif isinstance(v, tuple):\n",
    "                    config_dict[k] = list(v)\n",
    "                # 기본 타입은 그대로\n",
    "                elif isinstance(v, (str, int, float, bool, type(None))):\n",
    "                    config_dict[k] = v\n",
    "                # 나머지는 문자열로 변환\n",
    "                else:\n",
    "                    config_dict[k] = str(v)\n",
    "        return config_dict\n",
    "    \n",
    "    @classmethod\n",
    "    def save(cls, path: str = \"config.json\"):\n",
    "        \"\"\"설정을 JSON 파일로 저장\"\"\"\n",
    "        try:\n",
    "            config_dict = cls.to_dict()\n",
    "            with open(path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(config_dict, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"✓ 설정 저장: {path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ 설정 저장 실패: {e}\")\n",
    "            # 저장 실패해도 프로그램은 계속 진행\n",
    "            return False\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: str = \"config.json\") -> dict:\n",
    "        \"\"\"설정을 JSON 파일로부터 로드\"\"\"\n",
    "        try:\n",
    "            if Path(path).exists():\n",
    "                with open(path, 'r', encoding='utf-8') as f:\n",
    "                    return json.load(f)\n",
    "            else:\n",
    "                print(f\"⚠ 설정 파일 없음: {path}\")\n",
    "                return {}\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ 설정 로드 실패: {e}\")\n",
    "            return {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c46a24c6-a65e-413b-aa8d-25cf660326c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. 한국어 전처리 함수\n",
    "# ============================================================================\n",
    "\n",
    "class KoreanPreprocessor:\n",
    "    \"\"\"한국어 텍스트 전처리\"\"\"\n",
    "    \n",
    "    # 특수 토큰 정의\n",
    "    SPECIAL_TOKENS = {\n",
    "        '<BOS>': '[BOS]',\n",
    "        '<EOS>': '[EOS]',\n",
    "        '<PAD>': '[PAD]',\n",
    "        '<UNK>': '[UNK]',\n",
    "    }\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_text(text: str) -> str:\n",
    "        \"\"\"한국어 텍스트 정제\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # 공백 정규화\n",
    "        text = text.strip()\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # 한글, 영문, 숫자, 기본 구두점만 유지\n",
    "        # ㄱ-ㅣ (한글 자모), ㄀-ㅟ (완성 한글), ㄰-ㅲ 포함\n",
    "        text = re.sub(\n",
    "            r'[^\\w\\s\\.\\!\\?\\,\\;\\:\\(\\)\\-\\'\\\"\\u3130-\\u318F\\uAC00-\\uD7AF]',\n",
    "            '',\n",
    "            text,\n",
    "            flags=re.UNICODE\n",
    "        )\n",
    "        \n",
    "        # 연속된 특수문자 정리\n",
    "        text = re.sub(r'[\\.\\!\\?\\,\\;]+', lambda m: m.group(0)[0] + ' ', text)\n",
    "        \n",
    "        # 다시 공백 정규화\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_sentence(text: str) -> str:\n",
    "        \"\"\"문장 전처리\"\"\"\n",
    "        text = KoreanPreprocessor.clean_text(text)\n",
    "        text = text.lower()  # 소문자 변환 (영문 부분)\n",
    "        \n",
    "        # 문장 끝에 마침표 추가 (없으면)\n",
    "        if text and text[-1] not in '.!?,;:':\n",
    "            text = text + '.'\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_data_from_csv(filepath: str, max_samples: int = None) -> List[Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        ChatbotData.csv 형식으로부터 데이터 로드\n",
    "        CSV 형식: Q (질문), A (답변), label (의도) 등\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(filepath, encoding='utf-8')\n",
    "            \n",
    "            # CSV 컬럼명 확인\n",
    "            print(f\"✓ 데이터 컬럼: {df.columns.tolist()}\")\n",
    "            \n",
    "            # Q, A 또는 question, answer 컬럼 찾기\n",
    "            q_col = None\n",
    "            a_col = None\n",
    "            \n",
    "            for col in df.columns:\n",
    "                if col.lower() in ['q', 'question']:\n",
    "                    q_col = col\n",
    "                elif col.lower() in ['a', 'answer']:\n",
    "                    a_col = col\n",
    "            \n",
    "            if q_col is None or a_col is None:\n",
    "                # 첫 두 컬럼 사용\n",
    "                q_col = df.columns[0]\n",
    "                a_col = df.columns[1]\n",
    "                print(f\"⚠ Q/A 컬럼 자동 선택: {q_col}, {a_col}\")\n",
    "            \n",
    "            # 데이터 로드\n",
    "            for idx, row in df.iterrows():\n",
    "                if max_samples and len(pairs) >= max_samples:\n",
    "                    break\n",
    "                \n",
    "                q = str(row[q_col]).strip()\n",
    "                a = str(row[a_col]).strip()\n",
    "                \n",
    "                # 유효성 검사\n",
    "                if q and a and len(q) > 1 and len(a) > 1:\n",
    "                    q_clean = KoreanPreprocessor.preprocess_sentence(q)\n",
    "                    a_clean = KoreanPreprocessor.preprocess_sentence(a)\n",
    "                    \n",
    "                    if q_clean and a_clean:\n",
    "                        pairs.append((q_clean, a_clean))\n",
    "            \n",
    "            print(f\"✓ {len(pairs)}개 Q&A 쌍 로드됨\")\n",
    "            return pairs\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"❌ 파일 없음: {filepath}\")\n",
    "            print(\"파일을 다음 경로에 놓아주세요:\")\n",
    "            print(f\"  {Path(filepath).absolute()}\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 데이터 로드 오류: {e}\")\n",
    "            return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8fe9004-a0b2-44c5-a87e-b774bf071734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. SentencePiece 토크나이저 설정\n",
    "# ============================================================================\n",
    "\n",
    "def train_tokenizer(pairs: List[Tuple[str, str]], config: Config):\n",
    "    \"\"\"SentencePiece 토크나이저 학습\"\"\"\n",
    "    \n",
    "    corpus_file = \"corpus.txt\"\n",
    "    \n",
    "    # 코퍼스 저장\n",
    "    with open(corpus_file, 'w', encoding='utf-8') as f:\n",
    "        for q, a in pairs:\n",
    "            f.write(q + '\\n')\n",
    "            f.write(a + '\\n')\n",
    "    \n",
    "    print(f\"✓ 코퍼스 저장: {corpus_file}\")\n",
    "    \n",
    "    # SentencePiece 학습\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        input=corpus_file,\n",
    "        model_prefix=\"spm_korean\",\n",
    "        vocab_size=config.VOCAB_SIZE_SPM,\n",
    "        character_coverage=1.0,\n",
    "        model_type=\"bpe\",\n",
    "        max_sentence_length=999999,\n",
    "        bos_id=1,\n",
    "        eos_id=2,\n",
    "        pad_id=0,\n",
    "        unk_id=3,\n",
    "        user_defined_symbols=['[SEP]'],\n",
    "        normalization_rule_name='identity',  # 한글 소문자 변환 방지\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ SentencePiece 토크나이저 학습 완료\")\n",
    "    \n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(\"spm_korean.model\")\n",
    "    \n",
    "    return sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "860600f3-6507-4734-8f78-cedebbc022b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. Positional Encoding\n",
    "# ============================================================================\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"위치 인코딩\"\"\"\n",
    "    \n",
    "    def __init__(self, position: int, d_model: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.position = position\n",
    "        \n",
    "        # 위치 인코딩 계산\n",
    "        pe = torch.zeros(position, d_model)\n",
    "        pos = torch.arange(0, position, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * \n",
    "            -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        if d_model % 2 == 1:\n",
    "            pe[:, 1::2] = torch.cos(pos * div_term[:-1])\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            x + positional encoding\n",
    "        \"\"\"\n",
    "        return x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85a6280a-45f9-425e-9b37-b76210939b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. Attention 메커니즘\n",
    "# ============================================================================\n",
    "\n",
    "def scaled_dot_product_attention(\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    mask: torch.Tensor = None,\n",
    "    dropout: nn.Module = None\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Scaled dot-product attention\n",
    "    \n",
    "    Args:\n",
    "        query: (batch_size, heads, seq_len, depth)\n",
    "        key: (batch_size, heads, seq_len, depth)\n",
    "        value: (batch_size, heads, seq_len, depth)\n",
    "        mask: attention mask\n",
    "        dropout: dropout layer\n",
    "    \n",
    "    Returns:\n",
    "        output, attention weights\n",
    "    \"\"\"\n",
    "    depth = key.size(-1)\n",
    "    \n",
    "    # 스케일된 내적\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(depth)\n",
    "    \n",
    "    # 마스킹\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # Softmax\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # 드롭아웃\n",
    "    if dropout is not None:\n",
    "        attention_weights = dropout(attention_weights)\n",
    "    \n",
    "    # 값과의 가중합\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"멀티헤드 어텐션\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model은 num_heads로 나누어떨어져야 함\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        self.query_dense = nn.Linear(d_model, d_model)\n",
    "        self.key_dense = nn.Linear(d_model, d_model)\n",
    "        self.value_dense = nn.Linear(d_model, d_model)\n",
    "        self.output_dense = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query, key, value: (batch_size, seq_len, d_model)\n",
    "            mask: attention mask\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projection\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "        \n",
    "        # 멀티헤드로 변형\n",
    "        query = query.view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n",
    "        key = key.view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n",
    "        \n",
    "        # Attention\n",
    "        attn_output, _ = scaled_dot_product_attention(\n",
    "            query, key, value, mask, self.dropout\n",
    "        )\n",
    "        \n",
    "        # 헤드 결합\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # 최종 linear projection\n",
    "        output = self.output_dense(attn_output)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f302f4c-b3ff-495d-a640-73573f0fe2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. 마스킹 함수\n",
    "# ============================================================================\n",
    "\n",
    "def create_padding_mask(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"패딩 마스크 생성\"\"\"\n",
    "    mask = (x == 0).float()\n",
    "    # ✅ 입력과 같은 device에서 처리\n",
    "    return mask.unsqueeze(1).unsqueeze(2).to(x.device)  # (batch, 1, 1, seq_len)\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Look-ahead 마스크 생성 (디코더용)\"\"\"\n",
    "    seq_len = x.size(1)\n",
    "    device = x.device  # ✅ 입력의 device 가져오기\n",
    "    \n",
    "    # ✅ 올바른 device에서 생성 (device 파라미터 지정)\n",
    "    look_ahead_mask = 1 - torch.tril(torch.ones((seq_len, seq_len), device=device))\n",
    "    \n",
    "    # ✅ padding_mask도 같은 device에서 생성됨\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    \n",
    "    # ✅ 모두 같은 device에서 처리\n",
    "    look_ahead_mask = look_ahead_mask.unsqueeze(0)\n",
    "    combined_mask = torch.maximum(look_ahead_mask, padding_mask)\n",
    "    \n",
    "    return (1 - combined_mask).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f6b7eee-a9fe-4df6-8aca-29ead856bc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 7. 인코더/디코더 레이어\n",
    "# ============================================================================\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"인코더 레이어\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, units: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units, d_model)\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        # Self-attention\n",
    "        attn_output = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.norm1(x + attn_output)\n",
    "        \n",
    "        # Feed-forward\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.norm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"디코더 레이어\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, units: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Self-attention\n",
    "        self.self_mha = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "        # Encoder-decoder attention\n",
    "        self.encdec_mha = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "        # Feed-forward\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units, d_model)\n",
    "        )\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.norm3 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        encoder_output: torch.Tensor,\n",
    "        look_ahead_mask: torch.Tensor = None,\n",
    "        padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        # Self-attention\n",
    "        self_attn = self.self_mha(x, x, x, look_ahead_mask)\n",
    "        self_attn = self.dropout1(self_attn)\n",
    "        out1 = self.norm1(x + self_attn)\n",
    "        \n",
    "        # Encoder-decoder attention\n",
    "        encdec_attn = self.encdec_mha(out1, encoder_output, encoder_output, padding_mask)\n",
    "        encdec_attn = self.dropout2(encdec_attn)\n",
    "        out2 = self.norm2(out1 + encdec_attn)\n",
    "        \n",
    "        # Feed-forward\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        out3 = self.norm3(out2 + ffn_output)\n",
    "        \n",
    "        return out3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb6315d3-d3f0-45d3-bdbc-b6cbf90e7d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 8. 인코더/디코더\n",
    "# ============================================================================\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"인코더\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        num_layers: int,\n",
    "        units: int,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(position=5000, d_model=d_model)\n",
    "        \n",
    "        self.enc_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, units, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        # 임베딩 및 포지셔널 인코딩\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 인코더 레이어 통과\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"디코더\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        num_layers: int,\n",
    "        units: int,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(position=5000, d_model=d_model)\n",
    "        \n",
    "        self.dec_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, units, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        encoder_output: torch.Tensor,\n",
    "        look_ahead_mask: torch.Tensor = None,\n",
    "        padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        # 임베딩 및 포지셔널 인코딩\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 디코더 레이어 통과\n",
    "        for layer in self.dec_layers:\n",
    "            x = layer(x, encoder_output, look_ahead_mask, padding_mask)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acbfa947-6ba5-45c0-9d0d-1e266ab21f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 9. 전체 Transformer 모델\n",
    "# ============================================================================\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"Transformer 모델\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        num_layers: int,\n",
    "        units: int,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(vocab_size, num_layers, units, d_model, num_heads, dropout)\n",
    "        self.decoder = Decoder(vocab_size, num_layers, units, d_model, num_heads, dropout)\n",
    "        \n",
    "        self.final_layer = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        encoder_input: torch.Tensor,\n",
    "        decoder_input: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # 마스킹\n",
    "        encoder_mask = None\n",
    "        \n",
    "        decoder_look_ahead_mask = create_look_ahead_mask(decoder_input)\n",
    "        decoder_padding_mask = None\n",
    "        \n",
    "        # 인코더\n",
    "        encoder_output = self.encoder(encoder_input, encoder_mask)\n",
    "        \n",
    "        # 디코더\n",
    "        decoder_output = self.decoder(\n",
    "            decoder_input,\n",
    "            encoder_output,\n",
    "            decoder_look_ahead_mask,\n",
    "            decoder_padding_mask\n",
    "        )\n",
    "        \n",
    "        # 출력층\n",
    "        logits = self.final_layer(decoder_output)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c255a501-fd73-44dd-8d56-9cc69703bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 10. 데이터셋\n",
    "# ============================================================================\n",
    "\n",
    "class ChatbotDataset(Dataset):\n",
    "    \"\"\"챗봇 데이터셋\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        pairs: List[Tuple[str, str]],\n",
    "        tokenizer,\n",
    "        max_length: int = 50\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "        \n",
    "        for q, a in pairs:\n",
    "            # 토크나이징\n",
    "            q_ids = self.tokenizer.EncodeAsIds(q)\n",
    "            a_ids = self.tokenizer.EncodeAsIds(a)\n",
    "            \n",
    "            # 길이 조정\n",
    "            q_ids = self._pad_or_truncate(q_ids, max_length)\n",
    "            a_ids = self._pad_or_truncate(a_ids, max_length)\n",
    "            \n",
    "            # 디코더 입력: [BOS] + answer\n",
    "            dec_input = [self.tokenizer.bos_id()] + a_ids[:-1]\n",
    "            dec_input = self._pad_or_truncate(dec_input, max_length)\n",
    "            \n",
    "            # 디코더 레이블: answer + [EOS]\n",
    "            dec_label = a_ids[:]\n",
    "            dec_label = self._pad_or_truncate(dec_label, max_length)\n",
    "            \n",
    "            self.data.append((\n",
    "                torch.tensor(q_ids, dtype=torch.long),\n",
    "                torch.tensor(dec_input, dtype=torch.long),\n",
    "                torch.tensor(dec_label, dtype=torch.long)\n",
    "            ))\n",
    "    \n",
    "    def _pad_or_truncate(self, ids: List[int], max_len: int) -> List[int]:\n",
    "        \"\"\"패딩 또는 자르기\"\"\"\n",
    "        if len(ids) >= max_len:\n",
    "            return ids[:max_len]\n",
    "        else:\n",
    "            return ids + [self.tokenizer.pad_id()] * (max_len - len(ids))\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        return self.data[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efbd6fa9-4bed-4e2d-91a7-a256a0bd39b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 11. 손실 함수 (라벨 스무딩 포함)\n",
    "# ============================================================================\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    \"\"\"라벨 스무딩을 포함한 손실 함수\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, padding_idx: int = 0, smoothing: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.padding_idx = padding_idx\n",
    "        self.smoothing = smoothing\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        \n",
    "        self.criterion = nn.KLDivLoss(reduction='none')\n",
    "    \n",
    "    def forward(self, logits: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: (batch_size, seq_len, vocab_size)\n",
    "            target: (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            loss: scalar\n",
    "        \"\"\"\n",
    "        # Log softmax\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        # 타겟을 원-핫 인코딩\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(log_probs)\n",
    "            true_dist.scatter_(-1, target.unsqueeze(-1), self.confidence)\n",
    "            true_dist += self.smoothing / (self.vocab_size - 1)\n",
    "            \n",
    "            # 패딩 토큰 제거\n",
    "            mask = (target != self.padding_idx).unsqueeze(-1).float()\n",
    "            true_dist = true_dist * mask\n",
    "        \n",
    "        # KL Divergence\n",
    "        loss = self.criterion(log_probs, true_dist).sum(dim=-1)\n",
    "        \n",
    "        # 평균 (패딩 제외)\n",
    "        mask = (target != self.padding_idx).float()\n",
    "        return (loss * mask).sum() / mask.sum().clamp(min=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9d8c2db-5e4e-48a8-9e34-b316ef6f1aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 12. 메트릭\n",
    "# ============================================================================\n",
    "\n",
    "def accuracy_function(logits: torch.Tensor, targets: torch.Tensor, pad_id: int = 0) -> float:\n",
    "    \"\"\"정확도 계산\"\"\"\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    mask = (targets != pad_id).float()\n",
    "    correct = (preds == targets).float() * mask\n",
    "    accuracy = correct.sum() / mask.sum().clamp(min=1)\n",
    "    return accuracy.item()\n",
    "\n",
    "\n",
    "def perplexity_function(loss: float) -> float:\n",
    "    \"\"\"Perplexity 계산\"\"\"\n",
    "    return np.exp(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "662662d4-eb8a-45f1-87f6-f14841001cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 13. 학습 함수\n",
    "# ============================================================================\n",
    "\n",
    "def get_lr_lambda(d_model: int, warmup_steps: int = 4000):\n",
    "    \"\"\"학습률 스케줄\"\"\"\n",
    "    def lr_lambda(step: int) -> float:\n",
    "        step = float(step + 1)\n",
    "        warmup_steps_f = float(warmup_steps)\n",
    "        d_model_f = float(d_model)\n",
    "        return (d_model_f ** -0.5) * min(\n",
    "            step ** -0.5,\n",
    "            step * (warmup_steps_f ** -1.5)\n",
    "        )\n",
    "    return lr_lambda\n",
    "\n",
    "\n",
    "def train_step(\n",
    "    model: nn.Module,\n",
    "    batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n",
    "    optimizer: optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "    scaler: GradScaler = None\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"한 스텝 학습\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    enc_input, dec_input, target = [x.to(device) for x in batch]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 혼합 정밀도 학습\n",
    "    if scaler:\n",
    "        with autocast():\n",
    "            logits = model(enc_input, dec_input)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)).to(device), target.view(-1).to(device))\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    else:\n",
    "        logits = model(enc_input, dec_input)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)).to(device), target.view(-1).to(device))\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "    \n",
    "    acc = accuracy_function(logits, target, pad_id=0)\n",
    "    \n",
    "    return loss.item(), acc\n",
    "\n",
    "\n",
    "def validate(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"검증\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            enc_input, dec_input, target = [x.to(device) for x in batch]\n",
    "            \n",
    "            logits = model(enc_input, dec_input)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)).to(device), target.view(-1).to(device))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_acc += accuracy_function(logits, target, pad_id=0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_acc = total_acc / len(dataloader)\n",
    "    \n",
    "    return avg_loss, avg_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d670336-2641-4a4f-8bac-b6859e5fa038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 14. 추론 함수 (Beam Search)\n",
    "# ============================================================================\n",
    "\n",
    "def beam_search_decode(\n",
    "    model: nn.Module,\n",
    "    encoder_input: torch.Tensor,\n",
    "    tokenizer,\n",
    "    device: torch.device,\n",
    "    beam_size: int = 5,\n",
    "    max_length: int = 50,\n",
    "    length_penalty: float = 0.6\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Beam search를 사용한 디코딩\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    START_TOKEN = tokenizer.bos_id()\n",
    "    END_TOKEN = tokenizer.eos_id()\n",
    "    \n",
    "    # 인코더 처리\n",
    "    with torch.no_grad():\n",
    "        encoder_output = model.encoder(encoder_input)\n",
    "    \n",
    "    # 초기 후보\n",
    "    candidates = [(0.0, [START_TOKEN])]\n",
    "    \n",
    "    for step in range(max_length - 1):\n",
    "        all_candidates = []\n",
    "        \n",
    "        for log_prob, sequence in candidates:\n",
    "            # 디코더 입력\n",
    "            decoder_input = torch.tensor([sequence], device=device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                decoder_output = model.decoder(\n",
    "                    decoder_input,\n",
    "                    encoder_output,\n",
    "                    create_look_ahead_mask(decoder_input),\n",
    "                    None\n",
    "                )\n",
    "                logits = decoder_output[0, -1, :]\n",
    "                log_probs = F.log_softmax(logits, dim=-1)\n",
    "            \n",
    "            # Top-k 선택\n",
    "            top_k_log_probs, top_k_indices = torch.topk(log_probs, min(beam_size * 2, log_probs.size(0)))\n",
    "            \n",
    "            # 후보 추가\n",
    "            for token_log_prob, token_id in zip(top_k_log_probs, top_k_indices):\n",
    "                token_id = token_id.item()\n",
    "                new_log_prob = log_prob + token_log_prob.item()\n",
    "                new_sequence = sequence + [token_id]\n",
    "                \n",
    "                # 길이 정규화\n",
    "                normalized_log_prob = new_log_prob / (len(new_sequence) ** length_penalty)\n",
    "                all_candidates.append((normalized_log_prob, new_sequence))\n",
    "        \n",
    "        # 상위 beam_size개 유지\n",
    "        all_candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "        candidates = all_candidates[:beam_size]\n",
    "        \n",
    "        # 모든 후보가 END_TOKEN으로 끝나면 종료\n",
    "        if all(seq[-1] == END_TOKEN for _, seq in candidates):\n",
    "            break\n",
    "    \n",
    "    best_sequence = candidates[0][1]\n",
    "    \n",
    "    return best_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e04fa79-457c-4e25-928b-639ea015b01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 15. 추론 함수 (온도 샘플링)\n",
    "# ============================================================================\n",
    "\n",
    "def top_k_sampling(logits: torch.Tensor, k: int = 40, temperature: float = 0.8) -> int:\n",
    "    \"\"\"Top-k 샘플링\"\"\"\n",
    "    logits = logits / temperature\n",
    "    top_k_logits, top_k_indices = torch.topk(logits, k)\n",
    "    \n",
    "    logits_filtered = torch.full_like(logits, float('-inf'))\n",
    "    logits_filtered[top_k_indices] = top_k_logits\n",
    "    \n",
    "    probs = F.softmax(logits_filtered, dim=-1)\n",
    "    token = torch.multinomial(probs, 1)\n",
    "    \n",
    "    return token.item()\n",
    "\n",
    "\n",
    "def nucleus_sampling(logits: torch.Tensor, p: float = 0.9, temperature: float = 0.8) -> int:\n",
    "    \"\"\"Nucleus (Top-p) 샘플링\"\"\"\n",
    "    logits = logits / temperature\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "    cumsum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    \n",
    "    sorted_indices_to_remove = cumsum_probs > p\n",
    "    sorted_indices_to_remove[0] = False\n",
    "    \n",
    "    sorted_probs[sorted_indices_to_remove] = 0.0\n",
    "    sorted_probs = sorted_probs / sorted_probs.sum()\n",
    "    \n",
    "    sampled_idx = torch.multinomial(sorted_probs, 1)\n",
    "    token = sorted_indices[sampled_idx].item()\n",
    "    \n",
    "    return token\n",
    "\n",
    "\n",
    "def generate_response(\n",
    "    model: nn.Module,\n",
    "    input_text: str,\n",
    "    tokenizer,\n",
    "    device: torch.device,\n",
    "    config: Config,\n",
    "    method: str = 'beam_search'\n",
    ") -> str:\n",
    "    \"\"\"응답 생성\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # 전처리\n",
    "    input_text = KoreanPreprocessor.preprocess_sentence(input_text)\n",
    "    \n",
    "    # 토크나이징\n",
    "    input_ids = [tokenizer.bos_id()] + tokenizer.EncodeAsIds(input_text) + [tokenizer.eos_id()]\n",
    "    input_ids = input_ids[:config.MAX_SEQ_LENGTH]\n",
    "    input_ids += [tokenizer.pad_id()] * (config.MAX_SEQ_LENGTH - len(input_ids))\n",
    "    \n",
    "    encoder_input = torch.tensor([input_ids], device=device)\n",
    "    \n",
    "    # 생성 방법 선택\n",
    "    if method == 'beam_search':\n",
    "        output_seq = beam_search_decode(\n",
    "            model, encoder_input, tokenizer, device,\n",
    "            beam_size=config.BEAM_SIZE,\n",
    "            length_penalty=config.LENGTH_PENALTY\n",
    "        )\n",
    "    elif method == 'temperature':\n",
    "        # 온도 샘플링\n",
    "        START_TOKEN = tokenizer.bos_id()\n",
    "        END_TOKEN = tokenizer.eos_id()\n",
    "        \n",
    "        output_seq = [START_TOKEN]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            encoder_output = model.encoder(encoder_input)\n",
    "            \n",
    "            for _ in range(config.MAX_SEQ_LENGTH - 1):\n",
    "                decoder_input = torch.tensor([output_seq], device=device)\n",
    "                decoder_output = model.decoder(\n",
    "                    decoder_input, encoder_output,\n",
    "                    create_look_ahead_mask(decoder_input), None\n",
    "                )\n",
    "                logits = decoder_output[0, -1, :]\n",
    "                \n",
    "                next_token = top_k_sampling(\n",
    "                    logits, k=config.TOP_K, temperature=config.TEMPERATURE\n",
    "                )\n",
    "                output_seq.append(next_token)\n",
    "                \n",
    "                if next_token == END_TOKEN:\n",
    "                    break\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    # 디코딩\n",
    "    response = tokenizer.decode(\n",
    "        [t for t in output_seq if t < tokenizer.GetPieceSize()]\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d3709a-8714-4bcf-ae95-c3f03eebd3c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "한국어 Transformer 챗봇 - 완전 통합\n",
      "======================================================================\n",
      "\n",
      "✓ 사용 기기: cuda\n",
      "✓ GPU: Tesla T4\n",
      "✓ GPU 메모리: 15.7GB\n",
      "✓ 설정 저장: config.json\n",
      "\n",
      "[1단계] 데이터 로드\n",
      "----------------------------------------------------------------------\n",
      "✓ 데이터 컬럼: ['Q', 'A', 'label']\n",
      "✓ 71630개 Q&A 쌍 로드됨\n",
      "✓ 71630개 Q&A 쌍 로드됨\n",
      "\n",
      "[2단계] SentencePiece 토크나이저 학습\n",
      "----------------------------------------------------------------------\n",
      "✓ 기존 토크나이저 로드\n",
      "\n",
      "[3단계] 데이터셋 준비\n",
      "----------------------------------------------------------------------\n",
      "✓ 학습 샘플: 64467, 검증 샘플: 7163\n",
      "\n",
      "[4단계] 모델 생성\n",
      "----------------------------------------------------------------------\n",
      "✓ 모델 파라미터: 117,677,888\n",
      "\n",
      "[5단계] 최적화 설정\n",
      "----------------------------------------------------------------------\n",
      "✓ 옵티마이저: AdamW\n",
      "✓ 손실 함수: Label Smoothing (smoothing=0.2)\n",
      "✓ 혼합 정밀도: 활성화\n",
      "\n",
      "[6단계] 모델 학습 시작\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[에폭 1/100]\n",
      "  Step    0/4030 | Loss: 6.7975 | Acc: 0.0000 | PPL: 895.58 | LR: 1.01e-11\n",
      "  Step  100/4030 | Loss: 6.8105 | Acc: 0.0000 | PPL: 907.31 | LR: 5.14e-10\n",
      "  Step  200/4030 | Loss: 6.8148 | Acc: 0.0000 | PPL: 911.25 | LR: 1.02e-09\n",
      "  Step  300/4030 | Loss: 6.8129 | Acc: 0.0000 | PPL: 909.48 | LR: 1.52e-09\n",
      "  Step  400/4030 | Loss: 6.8129 | Acc: 0.0000 | PPL: 909.51 | LR: 2.03e-09\n",
      "  Step  500/4030 | Loss: 6.8122 | Acc: 0.0000 | PPL: 908.87 | LR: 2.53e-09\n",
      "  Step  600/4030 | Loss: 6.8102 | Acc: 0.0000 | PPL: 907.03 | LR: 3.04e-09\n",
      "  Step  700/4030 | Loss: 6.8071 | Acc: 0.0001 | PPL: 904.27 | LR: 3.54e-09\n",
      "  Step  800/4030 | Loss: 6.8050 | Acc: 0.0001 | PPL: 902.31 | LR: 4.04e-09\n",
      "  Step  900/4030 | Loss: 6.8025 | Acc: 0.0000 | PPL: 900.10 | LR: 4.55e-09\n",
      "  Step 1000/4030 | Loss: 6.7993 | Acc: 0.0001 | PPL: 897.25 | LR: 5.05e-09\n",
      "  Step 1100/4030 | Loss: 6.7964 | Acc: 0.0001 | PPL: 894.62 | LR: 5.56e-09\n",
      "  Step 1200/4030 | Loss: 6.7933 | Acc: 0.0001 | PPL: 891.89 | LR: 6.06e-09\n",
      "  Step 1300/4030 | Loss: 6.7891 | Acc: 0.0001 | PPL: 888.11 | LR: 6.57e-09\n",
      "  Step 1400/4030 | Loss: 6.7856 | Acc: 0.0002 | PPL: 885.00 | LR: 7.07e-09\n",
      "  Step 1500/4030 | Loss: 6.7818 | Acc: 0.0002 | PPL: 881.64 | LR: 7.57e-09\n",
      "  Step 1600/4030 | Loss: 6.7771 | Acc: 0.0003 | PPL: 877.54 | LR: 8.08e-09\n",
      "  Step 1700/4030 | Loss: 6.7724 | Acc: 0.0003 | PPL: 873.38 | LR: 8.58e-09\n",
      "  Step 1800/4030 | Loss: 6.7679 | Acc: 0.0004 | PPL: 869.52 | LR: 9.09e-09\n",
      "  Step 1900/4030 | Loss: 6.7626 | Acc: 0.0007 | PPL: 864.93 | LR: 9.59e-09\n",
      "  Step 2000/4030 | Loss: 6.7577 | Acc: 0.0010 | PPL: 860.62 | LR: 1.01e-08\n",
      "  Step 2100/4030 | Loss: 6.7514 | Acc: 0.0015 | PPL: 855.26 | LR: 1.06e-08\n",
      "  Step 2200/4030 | Loss: 6.7451 | Acc: 0.0022 | PPL: 849.91 | LR: 1.11e-08\n",
      "  Step 2300/4030 | Loss: 6.7388 | Acc: 0.0031 | PPL: 844.56 | LR: 1.16e-08\n",
      "  Step 2400/4030 | Loss: 6.7326 | Acc: 0.0043 | PPL: 839.32 | LR: 1.21e-08\n",
      "  Step 2500/4030 | Loss: 6.7256 | Acc: 0.0058 | PPL: 833.45 | LR: 1.26e-08\n",
      "  Step 2600/4030 | Loss: 6.7184 | Acc: 0.0078 | PPL: 827.47 | LR: 1.31e-08\n",
      "  Step 2700/4030 | Loss: 6.7114 | Acc: 0.0101 | PPL: 821.75 | LR: 1.36e-08\n",
      "  Step 2800/4030 | Loss: 6.7038 | Acc: 0.0131 | PPL: 815.50 | LR: 1.41e-08\n",
      "  Step 2900/4030 | Loss: 6.6961 | Acc: 0.0165 | PPL: 809.25 | LR: 1.46e-08\n",
      "  Step 3000/4030 | Loss: 6.6883 | Acc: 0.0201 | PPL: 802.98 | LR: 1.51e-08\n",
      "  Step 3100/4030 | Loss: 6.6799 | Acc: 0.0241 | PPL: 796.21 | LR: 1.56e-08\n",
      "  Step 3200/4030 | Loss: 6.6717 | Acc: 0.0279 | PPL: 789.71 | LR: 1.61e-08\n",
      "  Step 3300/4030 | Loss: 6.6633 | Acc: 0.0319 | PPL: 783.15 | LR: 1.67e-08\n",
      "  Step 3400/4030 | Loss: 6.6545 | Acc: 0.0358 | PPL: 776.30 | LR: 1.72e-08\n",
      "  Step 3500/4030 | Loss: 6.6452 | Acc: 0.0396 | PPL: 769.06 | LR: 1.77e-08\n",
      "  Step 3600/4030 | Loss: 6.6360 | Acc: 0.0432 | PPL: 762.02 | LR: 1.82e-08\n",
      "  Step 3700/4030 | Loss: 6.6265 | Acc: 0.0466 | PPL: 754.85 | LR: 1.87e-08\n",
      "  Step 3800/4030 | Loss: 6.6168 | Acc: 0.0500 | PPL: 747.58 | LR: 1.92e-08\n",
      "  Step 3900/4030 | Loss: 6.6066 | Acc: 0.0532 | PPL: 739.97 | LR: 1.97e-08\n",
      "  Step 4000/4030 | Loss: 6.5970 | Acc: 0.0561 | PPL: 732.87 | LR: 2.02e-08\n",
      "  Train | Loss: 6.5940 | Acc: 0.0569 | PPL: 730.67\n",
      "  Valid | Loss: 6.0413 | Acc: 0.1746 | PPL: 420.43\n",
      "  ✓ 최고 모델 저장! (Val Loss: 6.0413)\n",
      "\n",
      "[에폭 2/100]\n",
      "  Step    0/4030 | Loss: 6.1872 | Acc: 0.1569 | PPL: 486.46 | LR: 2.03e-08\n",
      "  Step  100/4030 | Loss: 6.1793 | Acc: 0.1708 | PPL: 482.68 | LR: 2.08e-08\n",
      "  Step  200/4030 | Loss: 6.1634 | Acc: 0.1709 | PPL: 475.02 | LR: 2.13e-08\n",
      "  Step  300/4030 | Loss: 6.1455 | Acc: 0.1721 | PPL: 466.62 | LR: 2.18e-08\n",
      "  Step  400/4030 | Loss: 6.1323 | Acc: 0.1725 | PPL: 460.48 | LR: 2.24e-08\n",
      "  Step  500/4030 | Loss: 6.1211 | Acc: 0.1728 | PPL: 455.38 | LR: 2.29e-08\n",
      "  Step  600/4030 | Loss: 6.1074 | Acc: 0.1731 | PPL: 449.18 | LR: 2.34e-08\n",
      "  Step  700/4030 | Loss: 6.0941 | Acc: 0.1734 | PPL: 443.25 | LR: 2.39e-08\n",
      "  Step  800/4030 | Loss: 6.0799 | Acc: 0.1739 | PPL: 436.99 | LR: 2.44e-08\n",
      "  Step  900/4030 | Loss: 6.0681 | Acc: 0.1739 | PPL: 431.87 | LR: 2.49e-08\n",
      "  Step 1000/4030 | Loss: 6.0543 | Acc: 0.1741 | PPL: 425.93 | LR: 2.54e-08\n",
      "  Step 1100/4030 | Loss: 6.0420 | Acc: 0.1744 | PPL: 420.73 | LR: 2.59e-08\n",
      "  Step 1200/4030 | Loss: 6.0308 | Acc: 0.1744 | PPL: 416.04 | LR: 2.64e-08\n",
      "  Step 1300/4030 | Loss: 6.0206 | Acc: 0.1743 | PPL: 411.82 | LR: 2.69e-08\n",
      "  Step 1400/4030 | Loss: 6.0109 | Acc: 0.1741 | PPL: 407.84 | LR: 2.74e-08\n",
      "  Step 1500/4030 | Loss: 6.0020 | Acc: 0.1739 | PPL: 404.22 | LR: 2.79e-08\n",
      "  Step 1600/4030 | Loss: 5.9914 | Acc: 0.1741 | PPL: 399.97 | LR: 2.84e-08\n",
      "  Step 1700/4030 | Loss: 5.9818 | Acc: 0.1742 | PPL: 396.14 | LR: 2.89e-08\n",
      "  Step 1800/4030 | Loss: 5.9741 | Acc: 0.1741 | PPL: 393.12 | LR: 2.94e-08\n",
      "  Step 1900/4030 | Loss: 5.9655 | Acc: 0.1740 | PPL: 389.75 | LR: 2.99e-08\n",
      "  Step 2000/4030 | Loss: 5.9587 | Acc: 0.1739 | PPL: 387.09 | LR: 3.04e-08\n",
      "  Step 2100/4030 | Loss: 5.9512 | Acc: 0.1740 | PPL: 384.20 | LR: 3.09e-08\n",
      "  Step 2200/4030 | Loss: 5.9434 | Acc: 0.1741 | PPL: 381.22 | LR: 3.14e-08\n",
      "  Step 2300/4030 | Loss: 5.9356 | Acc: 0.1741 | PPL: 378.28 | LR: 3.19e-08\n",
      "  Step 2400/4030 | Loss: 5.9289 | Acc: 0.1741 | PPL: 375.73 | LR: 3.24e-08\n",
      "  Step 2500/4030 | Loss: 5.9230 | Acc: 0.1740 | PPL: 373.54 | LR: 3.29e-08\n",
      "  Step 2600/4030 | Loss: 5.9166 | Acc: 0.1740 | PPL: 371.16 | LR: 3.34e-08\n",
      "  Step 2700/4030 | Loss: 5.9100 | Acc: 0.1740 | PPL: 368.72 | LR: 3.39e-08\n",
      "  Step 2800/4030 | Loss: 5.9039 | Acc: 0.1741 | PPL: 366.45 | LR: 3.45e-08\n",
      "  Step 2900/4030 | Loss: 5.8986 | Acc: 0.1740 | PPL: 364.53 | LR: 3.50e-08\n",
      "  Step 3000/4030 | Loss: 5.8923 | Acc: 0.1740 | PPL: 362.25 | LR: 3.55e-08\n",
      "  Step 3100/4030 | Loss: 5.8860 | Acc: 0.1741 | PPL: 359.97 | LR: 3.60e-08\n",
      "  Step 3200/4030 | Loss: 5.8801 | Acc: 0.1741 | PPL: 357.84 | LR: 3.65e-08\n",
      "  Step 3300/4030 | Loss: 5.8742 | Acc: 0.1741 | PPL: 355.73 | LR: 3.70e-08\n",
      "  Step 3400/4030 | Loss: 5.8686 | Acc: 0.1741 | PPL: 353.77 | LR: 3.75e-08\n",
      "  Step 3500/4030 | Loss: 5.8635 | Acc: 0.1741 | PPL: 351.94 | LR: 3.80e-08\n",
      "  Step 3600/4030 | Loss: 5.8584 | Acc: 0.1740 | PPL: 350.15 | LR: 3.85e-08\n",
      "  Step 3700/4030 | Loss: 5.8531 | Acc: 0.1740 | PPL: 348.32 | LR: 3.90e-08\n",
      "  Step 3800/4030 | Loss: 5.8479 | Acc: 0.1740 | PPL: 346.50 | LR: 3.95e-08\n",
      "  Step 3900/4030 | Loss: 5.8429 | Acc: 0.1739 | PPL: 344.79 | LR: 4.00e-08\n",
      "  Step 4000/4030 | Loss: 5.8381 | Acc: 0.1739 | PPL: 343.14 | LR: 4.03e-08\n",
      "  Train | Loss: 5.8367 | Acc: 0.1739 | PPL: 342.65\n",
      "  Valid | Loss: 5.6299 | Acc: 0.1746 | PPL: 278.63\n",
      "  ✓ 최고 모델 저장! (Val Loss: 5.6299)\n",
      "\n",
      "[에폭 3/100]\n",
      "  Step    0/4030 | Loss: 5.8197 | Acc: 0.1619 | PPL: 336.86 | LR: 4.02e-08\n",
      "  Step  100/4030 | Loss: 5.6365 | Acc: 0.1714 | PPL: 280.49 | LR: 3.99e-08\n",
      "  Step  200/4030 | Loss: 5.6307 | Acc: 0.1723 | PPL: 278.87 | LR: 3.97e-08\n",
      "  Step  300/4030 | Loss: 5.6331 | Acc: 0.1718 | PPL: 279.53 | LR: 3.95e-08\n",
      "  Step  400/4030 | Loss: 5.6313 | Acc: 0.1728 | PPL: 279.03 | LR: 3.92e-08\n",
      "  Step  500/4030 | Loss: 5.6268 | Acc: 0.1734 | PPL: 277.78 | LR: 3.90e-08\n",
      "  Step  600/4030 | Loss: 5.6244 | Acc: 0.1731 | PPL: 277.09 | LR: 3.88e-08\n",
      "  Step  700/4030 | Loss: 5.6203 | Acc: 0.1733 | PPL: 275.98 | LR: 3.85e-08\n",
      "  Step  800/4030 | Loss: 5.6171 | Acc: 0.1733 | PPL: 275.08 | LR: 3.83e-08\n",
      "  Step  900/4030 | Loss: 5.6137 | Acc: 0.1733 | PPL: 274.15 | LR: 3.81e-08\n",
      "  Step 1000/4030 | Loss: 5.6102 | Acc: 0.1731 | PPL: 273.21 | LR: 3.79e-08\n",
      "  Step 1100/4030 | Loss: 5.6062 | Acc: 0.1734 | PPL: 272.11 | LR: 3.77e-08\n",
      "  Step 1200/4030 | Loss: 5.6058 | Acc: 0.1733 | PPL: 272.01 | LR: 3.75e-08\n",
      "  Step 1300/4030 | Loss: 5.6034 | Acc: 0.1731 | PPL: 271.35 | LR: 3.73e-08\n",
      "  Step 1400/4030 | Loss: 5.5994 | Acc: 0.1734 | PPL: 270.27 | LR: 3.71e-08\n",
      "  Step 1500/4030 | Loss: 5.5965 | Acc: 0.1734 | PPL: 269.48 | LR: 3.69e-08\n",
      "  Step 1600/4030 | Loss: 5.5928 | Acc: 0.1735 | PPL: 268.50 | LR: 3.67e-08\n",
      "  Step 1700/4030 | Loss: 5.5904 | Acc: 0.1736 | PPL: 267.85 | LR: 3.65e-08\n",
      "  Step 1800/4030 | Loss: 5.5877 | Acc: 0.1735 | PPL: 267.12 | LR: 3.63e-08\n",
      "  Step 1900/4030 | Loss: 5.5842 | Acc: 0.1737 | PPL: 266.19 | LR: 3.62e-08\n",
      "  Step 2000/4030 | Loss: 5.5827 | Acc: 0.1736 | PPL: 265.78 | LR: 3.60e-08\n",
      "  Step 2100/4030 | Loss: 5.5796 | Acc: 0.1736 | PPL: 264.97 | LR: 3.58e-08\n",
      "  Step 2200/4030 | Loss: 5.5776 | Acc: 0.1736 | PPL: 264.43 | LR: 3.56e-08\n",
      "  Step 2300/4030 | Loss: 5.5743 | Acc: 0.1737 | PPL: 263.56 | LR: 3.54e-08\n",
      "  Step 2400/4030 | Loss: 5.5714 | Acc: 0.1740 | PPL: 262.80 | LR: 3.53e-08\n",
      "  Step 2500/4030 | Loss: 5.5687 | Acc: 0.1739 | PPL: 262.09 | LR: 3.51e-08\n",
      "  Step 2600/4030 | Loss: 5.5661 | Acc: 0.1738 | PPL: 261.41 | LR: 3.49e-08\n",
      "  Step 2700/4030 | Loss: 5.5644 | Acc: 0.1738 | PPL: 260.97 | LR: 3.48e-08\n",
      "  Step 2800/4030 | Loss: 5.5624 | Acc: 0.1737 | PPL: 260.44 | LR: 3.46e-08\n",
      "  Step 2900/4030 | Loss: 5.5595 | Acc: 0.1738 | PPL: 259.70 | LR: 3.45e-08\n",
      "  Step 3000/4030 | Loss: 5.5572 | Acc: 0.1738 | PPL: 259.09 | LR: 3.43e-08\n",
      "  Step 3100/4030 | Loss: 5.5551 | Acc: 0.1739 | PPL: 258.55 | LR: 3.42e-08\n",
      "  Step 3200/4030 | Loss: 5.5525 | Acc: 0.1739 | PPL: 257.89 | LR: 3.40e-08\n",
      "  Step 3300/4030 | Loss: 5.5503 | Acc: 0.1739 | PPL: 257.31 | LR: 3.39e-08\n",
      "  Step 3400/4030 | Loss: 5.5481 | Acc: 0.1739 | PPL: 256.76 | LR: 3.37e-08\n",
      "  Step 3500/4030 | Loss: 5.5459 | Acc: 0.1739 | PPL: 256.17 | LR: 3.36e-08\n",
      "  Step 3600/4030 | Loss: 5.5432 | Acc: 0.1740 | PPL: 255.50 | LR: 3.34e-08\n",
      "  Step 3700/4030 | Loss: 5.5414 | Acc: 0.1740 | PPL: 255.05 | LR: 3.33e-08\n",
      "  Step 3800/4030 | Loss: 5.5400 | Acc: 0.1739 | PPL: 254.67 | LR: 3.31e-08\n",
      "  Step 3900/4030 | Loss: 5.5382 | Acc: 0.1739 | PPL: 254.23 | LR: 3.30e-08\n",
      "  Step 4000/4030 | Loss: 5.5363 | Acc: 0.1739 | PPL: 253.73 | LR: 3.29e-08\n",
      "  Train | Loss: 5.5356 | Acc: 0.1739 | PPL: 253.56\n",
      "  Valid | Loss: 5.4047 | Acc: 0.1746 | PPL: 222.44\n",
      "  ✓ 최고 모델 저장! (Val Loss: 5.4047)\n",
      "\n",
      "[에폭 4/100]\n",
      "  Step    0/4030 | Loss: 5.4878 | Acc: 0.1702 | PPL: 241.73 | LR: 3.28e-08\n",
      "  Step  100/4030 | Loss: 5.4678 | Acc: 0.1715 | PPL: 236.95 | LR: 3.27e-08\n",
      "  Step  200/4030 | Loss: 5.4467 | Acc: 0.1740 | PPL: 232.00 | LR: 3.25e-08\n",
      "  Step  300/4030 | Loss: 5.4502 | Acc: 0.1742 | PPL: 232.80 | LR: 3.24e-08\n",
      "  Step  400/4030 | Loss: 5.4478 | Acc: 0.1743 | PPL: 232.26 | LR: 3.23e-08\n",
      "  Step  500/4030 | Loss: 5.4462 | Acc: 0.1743 | PPL: 231.86 | LR: 3.22e-08\n",
      "  Step  600/4030 | Loss: 5.4479 | Acc: 0.1741 | PPL: 232.28 | LR: 3.20e-08\n",
      "  Step  700/4030 | Loss: 5.4475 | Acc: 0.1743 | PPL: 232.17 | LR: 3.19e-08\n",
      "  Step  800/4030 | Loss: 5.4476 | Acc: 0.1739 | PPL: 232.20 | LR: 3.18e-08\n",
      "  Step  900/4030 | Loss: 5.4496 | Acc: 0.1738 | PPL: 232.66 | LR: 3.17e-08\n",
      "  Step 1000/4030 | Loss: 5.4504 | Acc: 0.1740 | PPL: 232.86 | LR: 3.15e-08\n",
      "  Step 1100/4030 | Loss: 5.4489 | Acc: 0.1741 | PPL: 232.51 | LR: 3.14e-08\n",
      "  Step 1200/4030 | Loss: 5.4469 | Acc: 0.1740 | PPL: 232.04 | LR: 3.13e-08\n",
      "  Step 1300/4030 | Loss: 5.4457 | Acc: 0.1740 | PPL: 231.75 | LR: 3.12e-08\n",
      "  Step 1400/4030 | Loss: 5.4438 | Acc: 0.1739 | PPL: 231.32 | LR: 3.11e-08\n",
      "  Step 1500/4030 | Loss: 5.4415 | Acc: 0.1739 | PPL: 230.78 | LR: 3.10e-08\n",
      "  Step 1600/4030 | Loss: 5.4390 | Acc: 0.1740 | PPL: 230.22 | LR: 3.08e-08\n",
      "  Step 1700/4030 | Loss: 5.4372 | Acc: 0.1740 | PPL: 229.81 | LR: 3.07e-08\n",
      "  Step 1800/4030 | Loss: 5.4368 | Acc: 0.1739 | PPL: 229.71 | LR: 3.06e-08\n",
      "  Step 1900/4030 | Loss: 5.4346 | Acc: 0.1739 | PPL: 229.21 | LR: 3.05e-08\n",
      "  Step 2000/4030 | Loss: 5.4322 | Acc: 0.1740 | PPL: 228.64 | LR: 3.04e-08\n",
      "  Step 2100/4030 | Loss: 5.4320 | Acc: 0.1739 | PPL: 228.61 | LR: 3.03e-08\n",
      "  Step 2200/4030 | Loss: 5.4302 | Acc: 0.1739 | PPL: 228.18 | LR: 3.02e-08\n",
      "  Step 2300/4030 | Loss: 5.4295 | Acc: 0.1739 | PPL: 228.04 | LR: 3.01e-08\n",
      "  Step 2400/4030 | Loss: 5.4284 | Acc: 0.1739 | PPL: 227.78 | LR: 3.00e-08\n",
      "  Step 2500/4030 | Loss: 5.4269 | Acc: 0.1739 | PPL: 227.44 | LR: 2.99e-08\n",
      "  Step 2600/4030 | Loss: 5.4268 | Acc: 0.1739 | PPL: 227.42 | LR: 2.98e-08\n",
      "  Step 2700/4030 | Loss: 5.4260 | Acc: 0.1739 | PPL: 227.24 | LR: 2.97e-08\n",
      "  Step 2800/4030 | Loss: 5.4252 | Acc: 0.1740 | PPL: 227.06 | LR: 2.96e-08\n",
      "  Step 2900/4030 | Loss: 5.4239 | Acc: 0.1739 | PPL: 226.77 | LR: 2.95e-08\n",
      "  Step 3000/4030 | Loss: 5.4234 | Acc: 0.1739 | PPL: 226.64 | LR: 2.94e-08\n",
      "  Step 3100/4030 | Loss: 5.4227 | Acc: 0.1738 | PPL: 226.48 | LR: 2.93e-08\n",
      "  Step 3200/4030 | Loss: 5.4213 | Acc: 0.1738 | PPL: 226.16 | LR: 2.92e-08\n",
      "  Step 3300/4030 | Loss: 5.4208 | Acc: 0.1740 | PPL: 226.06 | LR: 2.91e-08\n",
      "  Step 3400/4030 | Loss: 5.4201 | Acc: 0.1740 | PPL: 225.91 | LR: 2.90e-08\n",
      "  Step 3500/4030 | Loss: 5.4190 | Acc: 0.1739 | PPL: 225.66 | LR: 2.89e-08\n",
      "  Step 3600/4030 | Loss: 5.4180 | Acc: 0.1740 | PPL: 225.42 | LR: 2.88e-08\n",
      "  Step 3700/4030 | Loss: 5.4169 | Acc: 0.1739 | PPL: 225.19 | LR: 2.87e-08\n",
      "  Step 3800/4030 | Loss: 5.4156 | Acc: 0.1739 | PPL: 224.88 | LR: 2.86e-08\n",
      "  Step 3900/4030 | Loss: 5.4142 | Acc: 0.1740 | PPL: 224.57 | LR: 2.85e-08\n",
      "  Step 4000/4030 | Loss: 5.4131 | Acc: 0.1739 | PPL: 224.34 | LR: 2.84e-08\n",
      "  Train | Loss: 5.4126 | Acc: 0.1739 | PPL: 224.21\n",
      "  Valid | Loss: 5.3050 | Acc: 0.1748 | PPL: 201.34\n",
      "  ✓ 최고 모델 저장! (Val Loss: 5.3050)\n",
      "\n",
      "[에폭 5/100]\n",
      "  Step    0/4030 | Loss: 5.3212 | Acc: 0.1928 | PPL: 204.64 | LR: 2.84e-08\n",
      "  Step  100/4030 | Loss: 5.3698 | Acc: 0.1772 | PPL: 214.83 | LR: 2.83e-08\n",
      "  Step  200/4030 | Loss: 5.3521 | Acc: 0.1775 | PPL: 211.05 | LR: 2.82e-08\n",
      "  Step  300/4030 | Loss: 5.3629 | Acc: 0.1761 | PPL: 213.34 | LR: 2.82e-08\n",
      "  Step  400/4030 | Loss: 5.3609 | Acc: 0.1762 | PPL: 212.91 | LR: 2.81e-08\n",
      "  Step  500/4030 | Loss: 5.3621 | Acc: 0.1756 | PPL: 213.18 | LR: 2.80e-08\n",
      "  Step  600/4030 | Loss: 5.3617 | Acc: 0.1754 | PPL: 213.09 | LR: 2.79e-08\n",
      "  Step  700/4030 | Loss: 5.3630 | Acc: 0.1747 | PPL: 213.36 | LR: 2.78e-08\n",
      "  Step  800/4030 | Loss: 5.3626 | Acc: 0.1748 | PPL: 213.27 | LR: 2.77e-08\n",
      "  Step  900/4030 | Loss: 5.3619 | Acc: 0.1747 | PPL: 213.14 | LR: 2.77e-08\n",
      "  Step 1000/4030 | Loss: 5.3650 | Acc: 0.1744 | PPL: 213.80 | LR: 2.76e-08\n",
      "  Step 1100/4030 | Loss: 5.3636 | Acc: 0.1743 | PPL: 213.50 | LR: 2.75e-08\n",
      "  Step 1200/4030 | Loss: 5.3637 | Acc: 0.1743 | PPL: 213.52 | LR: 2.74e-08\n",
      "  Step 1300/4030 | Loss: 5.3629 | Acc: 0.1744 | PPL: 213.35 | LR: 2.73e-08\n",
      "  Step 1400/4030 | Loss: 5.3603 | Acc: 0.1744 | PPL: 212.78 | LR: 2.73e-08\n",
      "  Step 1500/4030 | Loss: 5.3576 | Acc: 0.1745 | PPL: 212.22 | LR: 2.72e-08\n",
      "  Step 1600/4030 | Loss: 5.3566 | Acc: 0.1744 | PPL: 211.99 | LR: 2.71e-08\n",
      "  Step 1700/4030 | Loss: 5.3568 | Acc: 0.1746 | PPL: 212.04 | LR: 2.70e-08\n",
      "  Step 1800/4030 | Loss: 5.3558 | Acc: 0.1747 | PPL: 211.84 | LR: 2.70e-08\n",
      "  Step 1900/4030 | Loss: 5.3559 | Acc: 0.1746 | PPL: 211.84 | LR: 2.69e-08\n",
      "  Step 2000/4030 | Loss: 5.3552 | Acc: 0.1746 | PPL: 211.72 | LR: 2.68e-08\n",
      "  Step 2100/4030 | Loss: 5.3541 | Acc: 0.1747 | PPL: 211.47 | LR: 2.67e-08\n",
      "  Step 2200/4030 | Loss: 5.3527 | Acc: 0.1746 | PPL: 211.18 | LR: 2.67e-08\n",
      "  Step 2300/4030 | Loss: 5.3531 | Acc: 0.1746 | PPL: 211.27 | LR: 2.66e-08\n",
      "  Step 2400/4030 | Loss: 5.3528 | Acc: 0.1746 | PPL: 211.19 | LR: 2.65e-08\n",
      "  Step 2500/4030 | Loss: 5.3518 | Acc: 0.1744 | PPL: 210.98 | LR: 2.64e-08\n",
      "  Step 2600/4030 | Loss: 5.3505 | Acc: 0.1744 | PPL: 210.72 | LR: 2.64e-08\n",
      "  Step 2700/4030 | Loss: 5.3499 | Acc: 0.1744 | PPL: 210.59 | LR: 2.63e-08\n",
      "  Step 2800/4030 | Loss: 5.3495 | Acc: 0.1743 | PPL: 210.50 | LR: 2.62e-08\n",
      "  Step 2900/4030 | Loss: 5.3481 | Acc: 0.1743 | PPL: 210.20 | LR: 2.62e-08\n",
      "  Step 3000/4030 | Loss: 5.3471 | Acc: 0.1742 | PPL: 209.99 | LR: 2.61e-08\n",
      "  Step 3100/4030 | Loss: 5.3475 | Acc: 0.1742 | PPL: 210.09 | LR: 2.60e-08\n",
      "  Step 3200/4030 | Loss: 5.3460 | Acc: 0.1742 | PPL: 209.76 | LR: 2.60e-08\n",
      "  Step 3300/4030 | Loss: 5.3450 | Acc: 0.1742 | PPL: 209.55 | LR: 2.59e-08\n",
      "  Step 3400/4030 | Loss: 5.3443 | Acc: 0.1742 | PPL: 209.42 | LR: 2.58e-08\n",
      "  Step 3500/4030 | Loss: 5.3441 | Acc: 0.1742 | PPL: 209.36 | LR: 2.58e-08\n",
      "  Step 3600/4030 | Loss: 5.3435 | Acc: 0.1742 | PPL: 209.24 | LR: 2.57e-08\n",
      "  Step 3700/4030 | Loss: 5.3435 | Acc: 0.1742 | PPL: 209.24 | LR: 2.56e-08\n",
      "  Step 3800/4030 | Loss: 5.3425 | Acc: 0.1742 | PPL: 209.03 | LR: 2.56e-08\n",
      "  Step 3900/4030 | Loss: 5.3427 | Acc: 0.1741 | PPL: 209.07 | LR: 2.55e-08\n",
      "  Step 4000/4030 | Loss: 5.3420 | Acc: 0.1740 | PPL: 208.94 | LR: 2.54e-08\n",
      "  Train | Loss: 5.3416 | Acc: 0.1740 | PPL: 208.84\n",
      "  Valid | Loss: 5.2399 | Acc: 0.1761 | PPL: 188.65\n",
      "  ✓ 최고 모델 저장! (Val Loss: 5.2399)\n",
      "\n",
      "[에폭 6/100]\n",
      "  Step    0/4030 | Loss: 5.7596 | Acc: 0.1553 | PPL: 317.23 | LR: 2.54e-08\n",
      "  Step  100/4030 | Loss: 5.3298 | Acc: 0.1728 | PPL: 206.40 | LR: 2.54e-08\n",
      "  Step  200/4030 | Loss: 5.3250 | Acc: 0.1730 | PPL: 205.40 | LR: 2.53e-08\n",
      "  Step  300/4030 | Loss: 5.3268 | Acc: 0.1736 | PPL: 205.77 | LR: 2.52e-08\n",
      "  Step  400/4030 | Loss: 5.3259 | Acc: 0.1728 | PPL: 205.59 | LR: 2.52e-08\n",
      "  Step  500/4030 | Loss: 5.3231 | Acc: 0.1733 | PPL: 205.02 | LR: 2.51e-08\n",
      "  Step  600/4030 | Loss: 5.3250 | Acc: 0.1734 | PPL: 205.41 | LR: 2.50e-08\n",
      "  Step  700/4030 | Loss: 5.3216 | Acc: 0.1736 | PPL: 204.71 | LR: 2.50e-08\n",
      "  Step  800/4030 | Loss: 5.3162 | Acc: 0.1736 | PPL: 203.60 | LR: 2.49e-08\n",
      "  Step  900/4030 | Loss: 5.3134 | Acc: 0.1738 | PPL: 203.04 | LR: 2.49e-08\n",
      "  Step 1000/4030 | Loss: 5.3113 | Acc: 0.1739 | PPL: 202.61 | LR: 2.48e-08\n",
      "  Step 1100/4030 | Loss: 5.3087 | Acc: 0.1741 | PPL: 202.09 | LR: 2.48e-08\n",
      "  Step 1200/4030 | Loss: 5.3080 | Acc: 0.1743 | PPL: 201.96 | LR: 2.47e-08\n",
      "  Step 1300/4030 | Loss: 5.3072 | Acc: 0.1743 | PPL: 201.78 | LR: 2.46e-08\n",
      "  Step 1400/4030 | Loss: 5.3070 | Acc: 0.1743 | PPL: 201.75 | LR: 2.46e-08\n",
      "  Step 1500/4030 | Loss: 5.3063 | Acc: 0.1743 | PPL: 201.60 | LR: 2.45e-08\n",
      "  Step 1600/4030 | Loss: 5.3038 | Acc: 0.1743 | PPL: 201.11 | LR: 2.45e-08\n",
      "  Step 1700/4030 | Loss: 5.3024 | Acc: 0.1745 | PPL: 200.82 | LR: 2.44e-08\n",
      "  Step 1800/4030 | Loss: 5.3003 | Acc: 0.1745 | PPL: 200.41 | LR: 2.44e-08\n",
      "  Step 1900/4030 | Loss: 5.2997 | Acc: 0.1745 | PPL: 200.29 | LR: 2.43e-08\n",
      "  Step 2000/4030 | Loss: 5.2993 | Acc: 0.1744 | PPL: 200.20 | LR: 2.42e-08\n",
      "  Step 2100/4030 | Loss: 5.2982 | Acc: 0.1743 | PPL: 199.98 | LR: 2.42e-08\n",
      "  Step 2200/4030 | Loss: 5.2978 | Acc: 0.1745 | PPL: 199.91 | LR: 2.41e-08\n",
      "  Step 2300/4030 | Loss: 5.2974 | Acc: 0.1744 | PPL: 199.82 | LR: 2.41e-08\n",
      "  Step 2400/4030 | Loss: 5.2954 | Acc: 0.1745 | PPL: 199.42 | LR: 2.40e-08\n",
      "  Step 2500/4030 | Loss: 5.2956 | Acc: 0.1744 | PPL: 199.46 | LR: 2.40e-08\n",
      "  Step 2600/4030 | Loss: 5.2947 | Acc: 0.1745 | PPL: 199.27 | LR: 2.39e-08\n",
      "  Step 2700/4030 | Loss: 5.2948 | Acc: 0.1745 | PPL: 199.30 | LR: 2.39e-08\n",
      "  Step 2800/4030 | Loss: 5.2941 | Acc: 0.1745 | PPL: 199.16 | LR: 2.38e-08\n",
      "  Step 2900/4030 | Loss: 5.2930 | Acc: 0.1745 | PPL: 198.93 | LR: 2.38e-08\n",
      "  Step 3000/4030 | Loss: 5.2929 | Acc: 0.1744 | PPL: 198.93 | LR: 2.37e-08\n",
      "  Step 3100/4030 | Loss: 5.2926 | Acc: 0.1746 | PPL: 198.86 | LR: 2.37e-08\n",
      "  Step 3200/4030 | Loss: 5.2929 | Acc: 0.1746 | PPL: 198.92 | LR: 2.36e-08\n",
      "  Step 3300/4030 | Loss: 5.2923 | Acc: 0.1745 | PPL: 198.80 | LR: 2.36e-08\n",
      "  Step 3400/4030 | Loss: 5.2917 | Acc: 0.1746 | PPL: 198.67 | LR: 2.35e-08\n",
      "  Step 3500/4030 | Loss: 5.2912 | Acc: 0.1745 | PPL: 198.59 | LR: 2.35e-08\n",
      "  Step 3600/4030 | Loss: 5.2908 | Acc: 0.1745 | PPL: 198.51 | LR: 2.34e-08\n",
      "  Step 3700/4030 | Loss: 5.2903 | Acc: 0.1746 | PPL: 198.40 | LR: 2.34e-08\n",
      "  Step 3800/4030 | Loss: 5.2900 | Acc: 0.1745 | PPL: 198.33 | LR: 2.33e-08\n",
      "  Step 3900/4030 | Loss: 5.2893 | Acc: 0.1745 | PPL: 198.20 | LR: 2.33e-08\n",
      "  Step 4000/4030 | Loss: 5.2890 | Acc: 0.1746 | PPL: 198.15 | LR: 2.32e-08\n",
      "  Train | Loss: 5.2888 | Acc: 0.1745 | PPL: 198.11\n",
      "  Valid | Loss: 5.1918 | Acc: 0.1803 | PPL: 179.80\n",
      "  ✓ 최고 모델 저장! (Val Loss: 5.1918)\n",
      "\n",
      "[에폭 7/100]\n",
      "  Step    0/4030 | Loss: 5.3678 | Acc: 0.1720 | PPL: 214.40 | LR: 2.32e-08\n",
      "  Step  100/4030 | Loss: 5.2860 | Acc: 0.1777 | PPL: 197.55 | LR: 2.32e-08\n",
      "  Step  200/4030 | Loss: 5.2649 | Acc: 0.1778 | PPL: 193.42 | LR: 2.31e-08\n",
      "  Step  300/4030 | Loss: 5.2682 | Acc: 0.1769 | PPL: 194.07 | LR: 2.31e-08\n",
      "  Step  400/4030 | Loss: 5.2626 | Acc: 0.1764 | PPL: 192.98 | LR: 2.30e-08\n",
      "  Step  500/4030 | Loss: 5.2670 | Acc: 0.1756 | PPL: 193.82 | LR: 2.30e-08\n",
      "  Step  600/4030 | Loss: 5.2665 | Acc: 0.1747 | PPL: 193.74 | LR: 2.29e-08\n",
      "  Step  700/4030 | Loss: 5.2640 | Acc: 0.1748 | PPL: 193.24 | LR: 2.29e-08\n",
      "  Step  800/4030 | Loss: 5.2700 | Acc: 0.1748 | PPL: 194.42 | LR: 2.28e-08\n",
      "  Step  900/4030 | Loss: 5.2653 | Acc: 0.1749 | PPL: 193.51 | LR: 2.28e-08\n",
      "  Step 1000/4030 | Loss: 5.2648 | Acc: 0.1748 | PPL: 193.41 | LR: 2.27e-08\n",
      "  Step 1100/4030 | Loss: 5.2626 | Acc: 0.1747 | PPL: 192.98 | LR: 2.27e-08\n",
      "  Step 1200/4030 | Loss: 5.2628 | Acc: 0.1746 | PPL: 193.02 | LR: 2.26e-08\n",
      "  Step 1300/4030 | Loss: 5.2607 | Acc: 0.1746 | PPL: 192.62 | LR: 2.26e-08\n",
      "  Step 1400/4030 | Loss: 5.2601 | Acc: 0.1748 | PPL: 192.50 | LR: 2.26e-08\n",
      "  Step 1500/4030 | Loss: 5.2606 | Acc: 0.1749 | PPL: 192.61 | LR: 2.25e-08\n",
      "  Step 1600/4030 | Loss: 5.2612 | Acc: 0.1749 | PPL: 192.72 | LR: 2.25e-08\n",
      "  Step 1700/4030 | Loss: 5.2584 | Acc: 0.1749 | PPL: 192.17 | LR: 2.24e-08\n",
      "  Step 1800/4030 | Loss: 5.2590 | Acc: 0.1749 | PPL: 192.29 | LR: 2.24e-08\n",
      "  Step 1900/4030 | Loss: 5.2578 | Acc: 0.1749 | PPL: 192.07 | LR: 2.23e-08\n",
      "  Step 2000/4030 | Loss: 5.2558 | Acc: 0.1749 | PPL: 191.68 | LR: 2.23e-08\n",
      "  Step 2100/4030 | Loss: 5.2556 | Acc: 0.1750 | PPL: 191.63 | LR: 2.23e-08\n",
      "  Step 2200/4030 | Loss: 5.2547 | Acc: 0.1750 | PPL: 191.46 | LR: 2.22e-08\n",
      "  Step 2300/4030 | Loss: 5.2544 | Acc: 0.1749 | PPL: 191.41 | LR: 2.22e-08\n",
      "  Step 2400/4030 | Loss: 5.2546 | Acc: 0.1751 | PPL: 191.44 | LR: 2.21e-08\n",
      "  Step 2500/4030 | Loss: 5.2542 | Acc: 0.1752 | PPL: 191.37 | LR: 2.21e-08\n",
      "  Step 2600/4030 | Loss: 5.2534 | Acc: 0.1752 | PPL: 191.21 | LR: 2.20e-08\n",
      "  Step 2700/4030 | Loss: 5.2541 | Acc: 0.1750 | PPL: 191.34 | LR: 2.20e-08\n",
      "  Step 2800/4030 | Loss: 5.2516 | Acc: 0.1750 | PPL: 190.88 | LR: 2.20e-08\n",
      "  Step 2900/4030 | Loss: 5.2517 | Acc: 0.1750 | PPL: 190.90 | LR: 2.19e-08\n",
      "  Step 3000/4030 | Loss: 5.2515 | Acc: 0.1749 | PPL: 190.86 | LR: 2.19e-08\n",
      "  Step 3100/4030 | Loss: 5.2517 | Acc: 0.1749 | PPL: 190.89 | LR: 2.18e-08\n",
      "  Step 3200/4030 | Loss: 5.2506 | Acc: 0.1750 | PPL: 190.69 | LR: 2.18e-08\n",
      "  Step 3300/4030 | Loss: 5.2500 | Acc: 0.1750 | PPL: 190.56 | LR: 2.18e-08\n",
      "  Step 3400/4030 | Loss: 5.2502 | Acc: 0.1750 | PPL: 190.60 | LR: 2.17e-08\n",
      "  Step 3500/4030 | Loss: 5.2492 | Acc: 0.1752 | PPL: 190.41 | LR: 2.17e-08\n",
      "  Step 3600/4030 | Loss: 5.2486 | Acc: 0.1752 | PPL: 190.30 | LR: 2.16e-08\n",
      "  Step 3700/4030 | Loss: 5.2483 | Acc: 0.1752 | PPL: 190.23 | LR: 2.16e-08\n",
      "  Step 3800/4030 | Loss: 5.2480 | Acc: 0.1753 | PPL: 190.19 | LR: 2.16e-08\n",
      "  Step 3900/4030 | Loss: 5.2475 | Acc: 0.1753 | PPL: 190.09 | LR: 2.15e-08\n",
      "  Step 4000/4030 | Loss: 5.2470 | Acc: 0.1754 | PPL: 189.99 | LR: 2.15e-08\n",
      "  Train | Loss: 5.2471 | Acc: 0.1754 | PPL: 190.01\n",
      "  Valid | Loss: 5.1516 | Acc: 0.1880 | PPL: 172.71\n",
      "  ✓ 최고 모델 저장! (Val Loss: 5.1516)\n",
      "\n",
      "[에폭 8/100]\n",
      "  Step    0/4030 | Loss: 4.9609 | Acc: 0.2222 | PPL: 142.72 | LR: 2.15e-08\n",
      "  Step  100/4030 | Loss: 5.2105 | Acc: 0.1791 | PPL: 183.19 | LR: 2.14e-08\n",
      "  Step  200/4030 | Loss: 5.2114 | Acc: 0.1780 | PPL: 183.35 | LR: 2.14e-08\n",
      "  Step  300/4030 | Loss: 5.2169 | Acc: 0.1775 | PPL: 184.37 | LR: 2.14e-08\n",
      "  Step  400/4030 | Loss: 5.2247 | Acc: 0.1772 | PPL: 185.80 | LR: 2.13e-08\n",
      "  Step  500/4030 | Loss: 5.2242 | Acc: 0.1777 | PPL: 185.72 | LR: 2.13e-08\n",
      "  Step  600/4030 | Loss: 5.2259 | Acc: 0.1772 | PPL: 186.04 | LR: 2.13e-08\n",
      "  Step  700/4030 | Loss: 5.2253 | Acc: 0.1772 | PPL: 185.92 | LR: 2.12e-08\n",
      "  Step  800/4030 | Loss: 5.2220 | Acc: 0.1773 | PPL: 185.31 | LR: 2.12e-08\n",
      "  Step  900/4030 | Loss: 5.2203 | Acc: 0.1774 | PPL: 184.99 | LR: 2.11e-08\n",
      "  Step 1000/4030 | Loss: 5.2213 | Acc: 0.1774 | PPL: 185.17 | LR: 2.11e-08\n",
      "  Step 1100/4030 | Loss: 5.2200 | Acc: 0.1774 | PPL: 184.93 | LR: 2.11e-08\n",
      "  Step 1200/4030 | Loss: 5.2193 | Acc: 0.1774 | PPL: 184.81 | LR: 2.10e-08\n",
      "  Step 1300/4030 | Loss: 5.2195 | Acc: 0.1772 | PPL: 184.85 | LR: 2.10e-08\n",
      "  Step 1400/4030 | Loss: 5.2211 | Acc: 0.1771 | PPL: 185.14 | LR: 2.10e-08\n",
      "  Step 1500/4030 | Loss: 5.2219 | Acc: 0.1769 | PPL: 185.29 | LR: 2.09e-08\n",
      "  Step 1600/4030 | Loss: 5.2212 | Acc: 0.1769 | PPL: 185.16 | LR: 2.09e-08\n",
      "  Step 1700/4030 | Loss: 5.2200 | Acc: 0.1768 | PPL: 184.93 | LR: 2.09e-08\n",
      "  Step 1800/4030 | Loss: 5.2214 | Acc: 0.1766 | PPL: 185.20 | LR: 2.08e-08\n",
      "  Step 1900/4030 | Loss: 5.2208 | Acc: 0.1766 | PPL: 185.08 | LR: 2.08e-08\n",
      "  Step 2000/4030 | Loss: 5.2205 | Acc: 0.1765 | PPL: 185.02 | LR: 2.08e-08\n",
      "  Step 2100/4030 | Loss: 5.2200 | Acc: 0.1767 | PPL: 184.93 | LR: 2.07e-08\n",
      "  Step 2200/4030 | Loss: 5.2179 | Acc: 0.1768 | PPL: 184.54 | LR: 2.07e-08\n",
      "  Step 2300/4030 | Loss: 5.2156 | Acc: 0.1768 | PPL: 184.12 | LR: 2.07e-08\n",
      "  Step 2400/4030 | Loss: 5.2146 | Acc: 0.1767 | PPL: 183.94 | LR: 2.06e-08\n",
      "  Step 2500/4030 | Loss: 5.2130 | Acc: 0.1768 | PPL: 183.65 | LR: 2.06e-08\n",
      "  Step 2600/4030 | Loss: 5.2128 | Acc: 0.1768 | PPL: 183.61 | LR: 2.06e-08\n",
      "  Step 2700/4030 | Loss: 5.2129 | Acc: 0.1769 | PPL: 183.62 | LR: 2.05e-08\n",
      "  Step 2800/4030 | Loss: 5.2126 | Acc: 0.1768 | PPL: 183.57 | LR: 2.05e-08\n",
      "  Step 2900/4030 | Loss: 5.2123 | Acc: 0.1769 | PPL: 183.51 | LR: 2.05e-08\n",
      "  Step 3000/4030 | Loss: 5.2121 | Acc: 0.1769 | PPL: 183.48 | LR: 2.04e-08\n",
      "  Step 3100/4030 | Loss: 5.2128 | Acc: 0.1769 | PPL: 183.61 | LR: 2.04e-08\n",
      "  Step 3200/4030 | Loss: 5.2124 | Acc: 0.1769 | PPL: 183.53 | LR: 2.04e-08\n",
      "  Step 3300/4030 | Loss: 5.2114 | Acc: 0.1769 | PPL: 183.35 | LR: 2.03e-08\n",
      "  Step 3400/4030 | Loss: 5.2109 | Acc: 0.1769 | PPL: 183.25 | LR: 2.03e-08\n",
      "  Step 3500/4030 | Loss: 5.2109 | Acc: 0.1768 | PPL: 183.27 | LR: 2.03e-08\n",
      "  Step 3600/4030 | Loss: 5.2118 | Acc: 0.1767 | PPL: 183.42 | LR: 2.02e-08\n",
      "  Step 3700/4030 | Loss: 5.2112 | Acc: 0.1768 | PPL: 183.31 | LR: 2.02e-08\n",
      "  Step 3800/4030 | Loss: 5.2109 | Acc: 0.1768 | PPL: 183.25 | LR: 2.02e-08\n",
      "  Step 3900/4030 | Loss: 5.2105 | Acc: 0.1768 | PPL: 183.19 | LR: 2.01e-08\n",
      "  Step 4000/4030 | Loss: 5.2108 | Acc: 0.1768 | PPL: 183.23 | LR: 2.01e-08\n",
      "  Train | Loss: 5.2105 | Acc: 0.1768 | PPL: 183.18\n",
      "  Valid | Loss: 5.1153 | Acc: 0.1929 | PPL: 166.54\n",
      "  ✓ 최고 모델 저장! (Val Loss: 5.1153)\n",
      "\n",
      "[에폭 9/100]\n",
      "  Step    0/4030 | Loss: 4.8422 | Acc: 0.2319 | PPL: 126.74 | LR: 2.01e-08\n",
      "  Step  100/4030 | Loss: 5.2081 | Acc: 0.1721 | PPL: 182.74 | LR: 2.01e-08\n",
      "  Step  200/4030 | Loss: 5.2014 | Acc: 0.1755 | PPL: 181.53 | LR: 2.00e-08\n",
      "  Step  300/4030 | Loss: 5.2032 | Acc: 0.1766 | PPL: 181.85 | LR: 2.00e-08\n",
      "  Step  400/4030 | Loss: 5.2026 | Acc: 0.1769 | PPL: 181.74 | LR: 2.00e-08\n",
      "  Step  500/4030 | Loss: 5.2021 | Acc: 0.1768 | PPL: 181.66 | LR: 1.99e-08\n",
      "  Step  600/4030 | Loss: 5.1947 | Acc: 0.1770 | PPL: 180.32 | LR: 1.99e-08\n",
      "  Step  700/4030 | Loss: 5.1917 | Acc: 0.1775 | PPL: 179.78 | LR: 1.99e-08\n",
      "  Step  800/4030 | Loss: 5.1883 | Acc: 0.1777 | PPL: 179.17 | LR: 1.99e-08\n",
      "  Step  900/4030 | Loss: 5.1901 | Acc: 0.1778 | PPL: 179.49 | LR: 1.98e-08\n",
      "  Step 1000/4030 | Loss: 5.1883 | Acc: 0.1782 | PPL: 179.16 | LR: 1.98e-08\n",
      "  Step 1100/4030 | Loss: 5.1872 | Acc: 0.1783 | PPL: 178.97 | LR: 1.98e-08\n",
      "  Step 1200/4030 | Loss: 5.1869 | Acc: 0.1784 | PPL: 178.91 | LR: 1.97e-08\n",
      "  Step 1300/4030 | Loss: 5.1859 | Acc: 0.1785 | PPL: 178.74 | LR: 1.97e-08\n",
      "  Step 1400/4030 | Loss: 5.1872 | Acc: 0.1785 | PPL: 178.96 | LR: 1.97e-08\n",
      "  Step 1500/4030 | Loss: 5.1858 | Acc: 0.1786 | PPL: 178.72 | LR: 1.96e-08\n",
      "  Step 1600/4030 | Loss: 5.1869 | Acc: 0.1786 | PPL: 178.91 | LR: 1.96e-08\n",
      "  Step 1700/4030 | Loss: 5.1882 | Acc: 0.1785 | PPL: 179.14 | LR: 1.96e-08\n",
      "  Step 1800/4030 | Loss: 5.1876 | Acc: 0.1787 | PPL: 179.03 | LR: 1.96e-08\n",
      "  Step 1900/4030 | Loss: 5.1870 | Acc: 0.1789 | PPL: 178.94 | LR: 1.95e-08\n",
      "  Step 2000/4030 | Loss: 5.1854 | Acc: 0.1790 | PPL: 178.64 | LR: 1.95e-08\n",
      "  Step 2100/4030 | Loss: 5.1874 | Acc: 0.1790 | PPL: 179.01 | LR: 1.95e-08\n",
      "  Step 2200/4030 | Loss: 5.1877 | Acc: 0.1789 | PPL: 179.06 | LR: 1.94e-08\n",
      "  Step 2300/4030 | Loss: 5.1865 | Acc: 0.1791 | PPL: 178.84 | LR: 1.94e-08\n",
      "  Step 2400/4030 | Loss: 5.1845 | Acc: 0.1792 | PPL: 178.48 | LR: 1.94e-08\n",
      "  Step 2500/4030 | Loss: 5.1843 | Acc: 0.1791 | PPL: 178.46 | LR: 1.94e-08\n",
      "  Step 2600/4030 | Loss: 5.1845 | Acc: 0.1790 | PPL: 178.48 | LR: 1.93e-08\n",
      "  Step 2700/4030 | Loss: 5.1829 | Acc: 0.1790 | PPL: 178.19 | LR: 1.93e-08\n",
      "  Step 2800/4030 | Loss: 5.1815 | Acc: 0.1789 | PPL: 177.95 | LR: 1.93e-08\n",
      "  Step 2900/4030 | Loss: 5.1810 | Acc: 0.1790 | PPL: 177.86 | LR: 1.92e-08\n",
      "  Step 3000/4030 | Loss: 5.1812 | Acc: 0.1789 | PPL: 177.90 | LR: 1.92e-08\n",
      "  Step 3100/4030 | Loss: 5.1815 | Acc: 0.1789 | PPL: 177.95 | LR: 1.92e-08\n",
      "  Step 3200/4030 | Loss: 5.1811 | Acc: 0.1789 | PPL: 177.88 | LR: 1.92e-08\n",
      "  Step 3300/4030 | Loss: 5.1814 | Acc: 0.1789 | PPL: 177.94 | LR: 1.91e-08\n",
      "  Step 3400/4030 | Loss: 5.1818 | Acc: 0.1788 | PPL: 178.00 | LR: 1.91e-08\n",
      "  Step 3500/4030 | Loss: 5.1814 | Acc: 0.1789 | PPL: 177.93 | LR: 1.91e-08\n",
      "  Step 3600/4030 | Loss: 5.1811 | Acc: 0.1788 | PPL: 177.88 | LR: 1.91e-08\n",
      "  Step 3700/4030 | Loss: 5.1808 | Acc: 0.1789 | PPL: 177.83 | LR: 1.90e-08\n",
      "  Step 3800/4030 | Loss: 5.1807 | Acc: 0.1789 | PPL: 177.80 | LR: 1.90e-08\n",
      "  Step 3900/4030 | Loss: 5.1800 | Acc: 0.1789 | PPL: 177.68 | LR: 1.90e-08\n",
      "  Step 4000/4030 | Loss: 5.1800 | Acc: 0.1789 | PPL: 177.68 | LR: 1.90e-08\n",
      "  Train | Loss: 5.1801 | Acc: 0.1789 | PPL: 177.70\n",
      "  Valid | Loss: 5.0840 | Acc: 0.1930 | PPL: 161.42\n",
      "  ✓ 최고 모델 저장! (Val Loss: 5.0840)\n",
      "\n",
      "[에폭 10/100]\n",
      "  Step    0/4030 | Loss: 5.2747 | Acc: 0.1616 | PPL: 195.33 | LR: 1.89e-08\n",
      "  Step  100/4030 | Loss: 5.1607 | Acc: 0.1809 | PPL: 174.28 | LR: 1.89e-08\n",
      "  Step  200/4030 | Loss: 5.1654 | Acc: 0.1809 | PPL: 175.10 | LR: 1.89e-08\n",
      "  Step  300/4030 | Loss: 5.1469 | Acc: 0.1820 | PPL: 171.90 | LR: 1.89e-08\n",
      "  Step  400/4030 | Loss: 5.1564 | Acc: 0.1809 | PPL: 173.54 | LR: 1.88e-08\n",
      "  Step  500/4030 | Loss: 5.1513 | Acc: 0.1815 | PPL: 172.66 | LR: 1.88e-08\n",
      "  Step  600/4030 | Loss: 5.1485 | Acc: 0.1809 | PPL: 172.17 | LR: 1.88e-08\n",
      "  Step  700/4030 | Loss: 5.1505 | Acc: 0.1805 | PPL: 172.52 | LR: 1.88e-08\n",
      "  Step  800/4030 | Loss: 5.1441 | Acc: 0.1806 | PPL: 171.41 | LR: 1.87e-08\n",
      "  Step  900/4030 | Loss: 5.1439 | Acc: 0.1809 | PPL: 171.39 | LR: 1.87e-08\n",
      "  Step 1000/4030 | Loss: 5.1457 | Acc: 0.1803 | PPL: 171.70 | LR: 1.87e-08\n",
      "  Step 1100/4030 | Loss: 5.1470 | Acc: 0.1806 | PPL: 171.91 | LR: 1.87e-08\n",
      "  Step 1200/4030 | Loss: 5.1473 | Acc: 0.1805 | PPL: 171.96 | LR: 1.86e-08\n",
      "  Step 1300/4030 | Loss: 5.1471 | Acc: 0.1804 | PPL: 171.93 | LR: 1.86e-08\n",
      "  Step 1400/4030 | Loss: 5.1469 | Acc: 0.1806 | PPL: 171.90 | LR: 1.86e-08\n",
      "  Step 1500/4030 | Loss: 5.1491 | Acc: 0.1807 | PPL: 172.27 | LR: 1.86e-08\n",
      "  Step 1600/4030 | Loss: 5.1492 | Acc: 0.1807 | PPL: 172.29 | LR: 1.85e-08\n",
      "  Step 1700/4030 | Loss: 5.1500 | Acc: 0.1808 | PPL: 172.43 | LR: 1.85e-08\n",
      "  Step 1800/4030 | Loss: 5.1510 | Acc: 0.1807 | PPL: 172.60 | LR: 1.85e-08\n",
      "  Step 1900/4030 | Loss: 5.1527 | Acc: 0.1808 | PPL: 172.90 | LR: 1.85e-08\n",
      "  Step 2000/4030 | Loss: 5.1526 | Acc: 0.1809 | PPL: 172.88 | LR: 1.84e-08\n",
      "  Step 2100/4030 | Loss: 5.1529 | Acc: 0.1809 | PPL: 172.92 | LR: 1.84e-08\n",
      "  Step 2200/4030 | Loss: 5.1526 | Acc: 0.1811 | PPL: 172.89 | LR: 1.84e-08\n",
      "  Step 2300/4030 | Loss: 5.1535 | Acc: 0.1811 | PPL: 173.03 | LR: 1.84e-08\n",
      "  Step 2400/4030 | Loss: 5.1535 | Acc: 0.1812 | PPL: 173.03 | LR: 1.83e-08\n",
      "  Step 2500/4030 | Loss: 5.1538 | Acc: 0.1811 | PPL: 173.09 | LR: 1.83e-08\n",
      "  Step 2600/4030 | Loss: 5.1525 | Acc: 0.1813 | PPL: 172.87 | LR: 1.83e-08\n",
      "  Step 2700/4030 | Loss: 5.1528 | Acc: 0.1813 | PPL: 172.91 | LR: 1.83e-08\n",
      "  Step 2800/4030 | Loss: 5.1526 | Acc: 0.1813 | PPL: 172.88 | LR: 1.83e-08\n",
      "  Step 2900/4030 | Loss: 5.1521 | Acc: 0.1813 | PPL: 172.80 | LR: 1.82e-08\n",
      "  Step 3000/4030 | Loss: 5.1513 | Acc: 0.1814 | PPL: 172.65 | LR: 1.82e-08\n",
      "  Step 3100/4030 | Loss: 5.1516 | Acc: 0.1814 | PPL: 172.71 | LR: 1.82e-08\n",
      "  Step 3200/4030 | Loss: 5.1510 | Acc: 0.1814 | PPL: 172.61 | LR: 1.82e-08\n",
      "  Step 3300/4030 | Loss: 5.1508 | Acc: 0.1814 | PPL: 172.57 | LR: 1.81e-08\n",
      "  Step 3400/4030 | Loss: 5.1501 | Acc: 0.1815 | PPL: 172.45 | LR: 1.81e-08\n",
      "  Step 3500/4030 | Loss: 5.1516 | Acc: 0.1815 | PPL: 172.71 | LR: 1.81e-08\n",
      "  Step 3600/4030 | Loss: 5.1511 | Acc: 0.1814 | PPL: 172.62 | LR: 1.81e-08\n",
      "  Step 3700/4030 | Loss: 5.1505 | Acc: 0.1814 | PPL: 172.52 | LR: 1.80e-08\n",
      "  Step 3800/4030 | Loss: 5.1512 | Acc: 0.1814 | PPL: 172.64 | LR: 1.80e-08\n",
      "  Step 3900/4030 | Loss: 5.1511 | Acc: 0.1814 | PPL: 172.63 | LR: 1.80e-08\n",
      "  Step 4000/4030 | Loss: 5.1514 | Acc: 0.1814 | PPL: 172.67 | LR: 1.80e-08\n",
      "  Train | Loss: 5.1511 | Acc: 0.1814 | PPL: 172.62\n",
      "  Valid | Loss: 5.0561 | Acc: 0.1931 | PPL: 156.97\n",
      "  ✓ 최고 모델 저장! (Val Loss: 5.0561)\n",
      "\n",
      "[에폭 11/100]\n",
      "  Step    0/4030 | Loss: 5.2674 | Acc: 0.1491 | PPL: 193.92 | LR: 1.80e-08\n",
      "  Step  100/4030 | Loss: 5.1247 | Acc: 0.1829 | PPL: 168.12 | LR: 1.80e-08\n",
      "  Step  200/4030 | Loss: 5.1288 | Acc: 0.1818 | PPL: 168.82 | LR: 1.79e-08\n",
      "  Step  300/4030 | Loss: 5.1184 | Acc: 0.1821 | PPL: 167.07 | LR: 1.79e-08\n",
      "  Step  400/4030 | Loss: 5.1211 | Acc: 0.1826 | PPL: 167.51 | LR: 1.79e-08\n",
      "  Step  500/4030 | Loss: 5.1308 | Acc: 0.1824 | PPL: 169.16 | LR: 1.79e-08\n",
      "  Step  600/4030 | Loss: 5.1311 | Acc: 0.1820 | PPL: 169.21 | LR: 1.78e-08\n",
      "  Step  700/4030 | Loss: 5.1316 | Acc: 0.1820 | PPL: 169.28 | LR: 1.78e-08\n",
      "  Step  800/4030 | Loss: 5.1355 | Acc: 0.1817 | PPL: 169.96 | LR: 1.78e-08\n",
      "  Step  900/4030 | Loss: 5.1350 | Acc: 0.1819 | PPL: 169.86 | LR: 1.78e-08\n",
      "  Step 1000/4030 | Loss: 5.1358 | Acc: 0.1817 | PPL: 169.99 | LR: 1.78e-08\n",
      "  Step 1100/4030 | Loss: 5.1361 | Acc: 0.1819 | PPL: 170.04 | LR: 1.77e-08\n",
      "  Step 1200/4030 | Loss: 5.1361 | Acc: 0.1819 | PPL: 170.04 | LR: 1.77e-08\n",
      "  Step 1300/4030 | Loss: 5.1360 | Acc: 0.1818 | PPL: 170.03 | LR: 1.77e-08\n",
      "  Step 1400/4030 | Loss: 5.1377 | Acc: 0.1817 | PPL: 170.32 | LR: 1.77e-08\n",
      "  Step 1500/4030 | Loss: 5.1370 | Acc: 0.1820 | PPL: 170.21 | LR: 1.76e-08\n",
      "  Step 1600/4030 | Loss: 5.1370 | Acc: 0.1820 | PPL: 170.21 | LR: 1.76e-08\n",
      "  Step 1700/4030 | Loss: 5.1365 | Acc: 0.1820 | PPL: 170.12 | LR: 1.76e-08\n",
      "  Step 1800/4030 | Loss: 5.1371 | Acc: 0.1821 | PPL: 170.22 | LR: 1.76e-08\n",
      "  Step 1900/4030 | Loss: 5.1358 | Acc: 0.1823 | PPL: 170.01 | LR: 1.76e-08\n",
      "  Step 2000/4030 | Loss: 5.1360 | Acc: 0.1824 | PPL: 170.04 | LR: 1.75e-08\n",
      "  Step 2100/4030 | Loss: 5.1367 | Acc: 0.1825 | PPL: 170.14 | LR: 1.75e-08\n",
      "  Step 2200/4030 | Loss: 5.1365 | Acc: 0.1827 | PPL: 170.11 | LR: 1.75e-08\n",
      "  Step 2300/4030 | Loss: 5.1347 | Acc: 0.1828 | PPL: 169.81 | LR: 1.75e-08\n",
      "  Step 2400/4030 | Loss: 5.1343 | Acc: 0.1828 | PPL: 169.75 | LR: 1.75e-08\n",
      "  Step 2500/4030 | Loss: 5.1345 | Acc: 0.1828 | PPL: 169.77 | LR: 1.74e-08\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 16. 메인 학습 루프\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 함수\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"한국어 Transformer 챗봇 - 완전 통합\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 시드 설정\n",
    "    torch.manual_seed(Config.SEED)\n",
    "    np.random.seed(Config.SEED)\n",
    "\n",
    "    # ✅ Device 설정 확인 및 고정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    Config.DEVICE = device  # Config의 DEVICE도 업데이트\n",
    "    \n",
    "    print(f\"\\n✓ 사용 기기: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"✓ GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "    \n",
    "    # 설정 저장 (실패해도 계속 진행)\n",
    "    try:\n",
    "        Config.save(\"config.json\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ 설정 저장 실패 (계속 진행): {e}\")\n",
    "    \n",
    "    # ========== 데이터 로드 ==========\n",
    "    print(\"\\n[1단계] 데이터 로드\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    pairs = KoreanPreprocessor.load_data_from_csv(\n",
    "        Config.DATA_PATH,\n",
    "        max_samples=Config.MAX_SAMPLES\n",
    "    )\n",
    "    \n",
    "    if not pairs:\n",
    "        print(\"❌ 데이터를 로드할 수 없습니다.\")\n",
    "        print(f\"다음 경로에 파일을 놓아주세요: {Path(Config.DATA_PATH).absolute()}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"✓ {len(pairs)}개 Q&A 쌍 로드됨\")\n",
    "    \n",
    "    # ========== 토크나이저 학습 ==========\n",
    "    print(\"\\n[2단계] SentencePiece 토크나이저 학습\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    if not Path(\"spm_korean.model\").exists():\n",
    "        tokenizer = train_tokenizer(pairs, Config)\n",
    "    else:\n",
    "        tokenizer = spm.SentencePieceProcessor()\n",
    "        tokenizer.Load(\"spm_korean.model\")\n",
    "        print(\"✓ 기존 토크나이저 로드\")\n",
    "    \n",
    "    # ========== 데이터셋 준비 ==========\n",
    "    print(\"\\n[3단계] 데이터셋 준비\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    dataset = ChatbotDataset(pairs, tokenizer, max_length=Config.MAX_SEQ_LENGTH)\n",
    "    \n",
    "    # 학습/검증 분할\n",
    "    train_size = int(len(dataset) * Config.TRAIN_VALID_SPLIT)\n",
    "    valid_size = len(dataset) - train_size\n",
    "    train_dataset, valid_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, valid_size]\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    print(f\"✓ 학습 샘플: {len(train_dataset)}, 검증 샘플: {len(valid_dataset)}\")\n",
    "    \n",
    "    # ========== 모델 생성 ==========\n",
    "    print(\"\\n[4단계] 모델 생성\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    model = Transformer(\n",
    "        vocab_size=tokenizer.GetPieceSize(),\n",
    "        num_layers=Config.NUM_LAYERS,\n",
    "        units=Config.UNITS,\n",
    "        d_model=Config.D_MODEL,\n",
    "        num_heads=Config.NUM_HEADS,\n",
    "        dropout=Config.DROPOUT\n",
    "    )\n",
    "    \n",
    "    model = model.to(Config.DEVICE)\n",
    "    \n",
    "    # 모델 파라미터 개수\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"✓ 모델 파라미터: {total_params:,}\")\n",
    "    \n",
    "    # ========== 옵티마이저 및 손실 함수 ==========\n",
    "    print(\"\\n[5단계] 최적화 설정\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=Config.LEARNING_RATE,\n",
    "        betas=Config.ADAM_BETAS,\n",
    "        weight_decay=Config.WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    scheduler = lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=get_lr_lambda(Config.D_MODEL, Config.WARMUP_STEPS)\n",
    "    )\n",
    "    \n",
    "    criterion = LabelSmoothingLoss(\n",
    "        vocab_size=tokenizer.GetPieceSize(),\n",
    "        padding_idx=tokenizer.pad_id(),\n",
    "        smoothing=Config.LABEL_SMOOTHING\n",
    "    )\n",
    "    \n",
    "    # ✅ 중요: criterion도 device로 이동\n",
    "    criterion = criterion.to(Config.DEVICE)\n",
    "    scaler = GradScaler() if Config.USE_MIXED_PRECISION else None\n",
    "    \n",
    "    print(f\"✓ 옵티마이저: AdamW\")\n",
    "    print(f\"✓ 손실 함수: Label Smoothing (smoothing={Config.LABEL_SMOOTHING})\")\n",
    "    print(f\"✓ 혼합 정밀도: {'활성화' if scaler else '비활성화'}\")\n",
    "    \n",
    "    # ========== 학습 ==========\n",
    "    print(\"\\n[6단계] 모델 학습 시작\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_count = 0\n",
    "    \n",
    "    for epoch in range(Config.NUM_EPOCHS):\n",
    "        print(f\"\\n[에폭 {epoch+1}/{Config.NUM_EPOCHS}]\")\n",
    "        \n",
    "        # 학습\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        \n",
    "        for step, batch in enumerate(train_loader):\n",
    "            loss, acc = train_step(model, batch, optimizer, criterion, Config.DEVICE, scaler)\n",
    "            \n",
    "            train_loss += loss\n",
    "            train_acc += acc\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            if step % Config.LOG_INTERVAL == 0:\n",
    "                avg_loss = train_loss / (step + 1)\n",
    "                avg_acc = train_acc / (step + 1)\n",
    "                ppl = perplexity_function(avg_loss)\n",
    "                lr = optimizer.param_groups[0]['lr']\n",
    "                \n",
    "                print(f\"  Step {step:4d}/{len(train_loader):4d} | \"\n",
    "                      f\"Loss: {avg_loss:.4f} | Acc: {avg_acc:.4f} | \"\n",
    "                      f\"PPL: {ppl:.2f} | LR: {lr:.2e}\")\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = train_acc / len(train_loader)\n",
    "        train_ppl = perplexity_function(train_loss)\n",
    "        \n",
    "        # 검증\n",
    "        val_loss, val_acc = validate(model, valid_loader, criterion, Config.DEVICE)\n",
    "        val_ppl = perplexity_function(val_loss)\n",
    "        \n",
    "        print(f\"  Train | Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | PPL: {train_ppl:.2f}\")\n",
    "        print(f\"  Valid | Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | PPL: {val_ppl:.2f}\")\n",
    "        \n",
    "        # 체크포인트 저장\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_count = 0\n",
    "            \n",
    "            checkpoint_path = Config.SAVE_DIR / f\"best_model.pt\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss': val_loss,\n",
    "                'config': Config.to_dict()\n",
    "            }, checkpoint_path)\n",
    "            print(f\"  ✓ 최고 모델 저장! (Val Loss: {val_loss:.4f})\")\n",
    "        else:\n",
    "            patience_count += 1\n",
    "            print(f\"  ⚠ Val Loss 개선 없음 ({patience_count}/{Config.EARLY_STOPPING_PATIENCE})\")\n",
    "            \n",
    "            if patience_count >= Config.EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\n✓ 조기 종료 (에폭 {epoch+1})\")\n",
    "                break\n",
    "    \n",
    "    # ========== 최고 모델 로드 ==========\n",
    "    print(\"\\n[7단계] 최고 모델 로드\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    best_checkpoint = torch.load(Config.SAVE_DIR / \"best_model.pt\", map_location=Config.DEVICE)\n",
    "    model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "    print(\"✓ 최고 모델 로드됨\")\n",
    "    \n",
    "    # ========== 테스트 ==========\n",
    "    print(\"\\n[8단계] 응답 생성 테스트\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    test_inputs = [\n",
    "        \"안녕하세요\",\n",
    "        \"오늘 날씨 어때요\",\n",
    "        \"감사합니다\",\n",
    "        \"뭐 하고 있어요\",\n",
    "    ]\n",
    "    \n",
    "    for input_text in test_inputs:\n",
    "        print(f\"\\n입력: {input_text}\")\n",
    "        \n",
    "        # Beam Search\n",
    "        response_beam = generate_response(\n",
    "            model, input_text, tokenizer, Config.DEVICE, Config, method='beam_search'\n",
    "        )\n",
    "        print(f\"응답(Beam): {response_beam}\")\n",
    "        \n",
    "        # 온도 샘플링\n",
    "        response_temp = generate_response(\n",
    "            model, input_text, tokenizer, Config.DEVICE, Config, method='temperature'\n",
    "        )\n",
    "        print(f\"응답(온도): {response_temp}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"✓ 학습 완료!\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85606570-0bb5-4514-9e61-b380bf48ce9a",
   "metadata": {},
   "source": [
    "### 1차 실험(데이터 추가 X), 2차 실험(질의 응답 코퍼스 추가)\n",
    "\n",
    "#### 1차 실험 결과 (100 에폭):\n",
    " - 손실 함수가 내려가는 양상은 보였으나 추론 진행하였을 때 상식적인 답변을 받지 못하였음 (valid Loss : 5.4742 | Acc : 0.1709)\n",
    " - 학습에 필요한 데이터 양이 적다고 판단하여 코퍼스 추가해서 2차 실험 판단\n",
    "\n",
    "#### 2차 실험 결과 (100 에폭):\n",
    " - 손실 함수가 꾸준히 내려가는 양상을 보이나 완료시간이 엄청 길어서 물리적인 시간이 많이 듦 ( 10 에폭 결과 > valid Loss : 5.0561 | Acc : 0.1931)\n",
    " - 확실히 데이터를 추가하니 같은 에폭을 돌렸을 때보다 성능이 높아 지는 것으로 판단\n",
    "\n",
    "종합 회고 : 복잡한 모델을 학습시키고 성능개선하기 상당히 많은 제약사항 (컴퓨팅 성능, 모델의 성능, 학습 데이터의 양, 데이터 전처리)을 경험하였고 좀더 실험을 체계적으로 세우고 세부적인 수정 및 개선하기위한 방법에 익숙해져야겠다는 생각이 든다 \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e754f2-ddc3-4a89-87a3-ffac2589d6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95499831-7f62-42c2-8424-df598f3d05b8",
   "metadata": {},
   "source": [
    "### 코퍼스 추가 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16f77c4-6702-4de8-bf3c-e107f0495ed1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "한국어 Q&A 코퍼스 통합 다운로더\n",
      "======================================================================\n",
      "\n",
      "📥 Step 1: ChatBot 데이터 다운로드 (GitHub)\n",
      "------------------------------------------------------------\n",
      "  🌐 https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\n",
      "  ✅ 다운로드 완료! (11,823 샘플)\n",
      "\n",
      "📥 Step 2: KorQA 데이터 파싱 (로컬)\n",
      "------------------------------------------------------------\n",
      "  📂 파일: korquad_v2.json\n",
      "  ✅ 파싱 완료! (60,407 샘플)\n",
      "\n",
      "⚠️  AI Hub 파일 없음 (선택사항)\n",
      "   다운로드: https://aihub.or.kr/\n",
      "   \"한국어 대화\" 검색 후 JSON 형식 다운로드\n",
      "   파일명을 aihub_data.json으로 변경하여 현재 디렉토리에 배치\n",
      "\n",
      "🔗 Step 4: 데이터 병합 및 저장\n",
      "------------------------------------------------------------\n",
      "  병합: 72,230 샘플\n",
      "  정제: 72,230 샘플 (빈 값 제거)\n",
      "  정제: 71,854 샘플 (길이 확인)\n",
      "  정제: 71,630 샘플 (224 중복 제거)\n",
      "  ✅ 저장: korean_qa_data.csv\n",
      "\n",
      "📊 샘플 데이터:\n",
      "======================================================================\n",
      "\n",
      "[1] (0)\n",
      "  Q: 12시 땡!\n",
      "  A: 하루가 또 가네요.\n",
      "\n",
      "[2] (0)\n",
      "  Q: 1지망 학교 떨어졌어\n",
      "  A: 위로해 드립니다.\n",
      "\n",
      "[3] (0)\n",
      "  Q: 3박4일 놀러가고 싶다\n",
      "  A: 여행은 언제나 좋죠.\n",
      "\n",
      "[4] (0)\n",
      "  Q: 3박4일 정도 놀러가고 싶다\n",
      "  A: 여행은 언제나 좋죠.\n",
      "\n",
      "[5] (0)\n",
      "  Q: PPL 심하네\n",
      "  A: 눈살이 찌푸려지죠.\n",
      "\n",
      "======================================================================\n",
      "✨ 완료!\n",
      "======================================================================\n",
      "\n",
      "📈 최종 통계:\n",
      "  파일: korean_qa_data.csv\n",
      "  샘플: 71,630 개\n",
      "  크기: 6.9MB\n",
      "  Q 평균 길이: 30.4 자\n",
      "  A 평균 길이: 7.2 자\n",
      "\n",
      "🚀 다음 단계:\n",
      "  1. merge_datasets.py 실행:\n",
      "     $ python3 merge_datasets.py\n",
      "\n",
      "  2. Config 수정:\n",
      "     DATA_PATH = 'korean_qa_data.csv'\n",
      "\n",
      "  3. 모델 재학습:\n",
      "     $ python3 korean_chatbot_complete.py\n",
      "\n",
      "======================================================================\n",
      "경고: merge_datasets.py를 사용할 때\n",
      "      korean_qa_data.csv를 ChatbotData.csv와 병합할 수 있습니다\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3680: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "한국어 Q&A 코퍼스 통합 다운로더\n",
    "\n",
    "지원하는 코퍼스:\n",
    "1. ChatBot 데이터 (GitHub) - 자동 다운로드\n",
    "2. KorQA (수동 다운로드 후 자동 처리)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "class KoreanQACorpusDownloader:\n",
    "    \"\"\"한국어 Q&A 코퍼스 통합 다운로더\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.output_file = \"korean_qa_data.csv\"\n",
    "        self.dfs = []\n",
    "    \n",
    "    def download_chatbot_data(self):\n",
    "        \"\"\"ChatBot 데이터 다운로드 (GitHub에서 자동)\"\"\"\n",
    "        print(\"\\n📥 Step 1: ChatBot 데이터 다운로드 (GitHub)\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        url = \"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\"\n",
    "        \n",
    "        print(f\"  🌐 {url}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(url, encoding='utf-8')\n",
    "            print(f\"  ✅ 다운로드 완료! ({len(df):,} 샘플)\")\n",
    "            \n",
    "            return df\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ 다운로드 실패: {e}\")\n",
    "            print(f\"     (인터넷 연결 확인)\")\n",
    "            return None\n",
    "    \n",
    "    def parse_local_korquad(self, json_file='korquad_v2.json'):\n",
    "        \"\"\"로컬 KorQA 파일 파싱\"\"\"\n",
    "        print(\"\\n📥 Step 2: KorQA 데이터 파싱 (로컬)\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        if not os.path.exists(json_file):\n",
    "            print(f\"  ⚠️ 파일 없음: {json_file}\")\n",
    "            print(f\"     다운로드: https://korquad.github.io/\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"  📂 파일: {json_file}\")\n",
    "        \n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            qa_pairs = []\n",
    "            \n",
    "            # KorQA 형식 파싱\n",
    "            for article in data.get('data', []):\n",
    "                for paragraph in article.get('paragraphs', []):\n",
    "                    for qa in paragraph.get('qas', []):\n",
    "                        question = qa.get('question', '')\n",
    "                        answers = qa.get('answers', [])\n",
    "                        \n",
    "                        if question and answers:\n",
    "                            answer = answers[0].get('text', '')\n",
    "                            qa_pairs.append({\n",
    "                                'Q': question,\n",
    "                                'A': answer,\n",
    "                                'label': 'korquad'\n",
    "                            })\n",
    "            \n",
    "            df = pd.DataFrame(qa_pairs)\n",
    "            print(f\"  ✅ 파싱 완료! ({len(df):,} 샘플)\")\n",
    "            \n",
    "            return df\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ 파싱 실패: {e}\")\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    def merge_and_save(self, *dataframes):\n",
    "        \"\"\"데이터 병합 및 저장\"\"\"\n",
    "        print(\"\\n🔗 Step 4: 데이터 병합 및 저장\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        dfs = [df for df in dataframes if df is not None]\n",
    "        \n",
    "        if not dfs:\n",
    "            print(\"  ❌ 병합할 데이터가 없습니다\")\n",
    "            return None\n",
    "        \n",
    "        # 병합\n",
    "        merged = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"  병합: {len(merged):,} 샘플\")\n",
    "        \n",
    "        # 정제\n",
    "        # 1. 빈 값 제거\n",
    "        merged = merged.dropna()\n",
    "        print(f\"  정제: {len(merged):,} 샘플 (빈 값 제거)\")\n",
    "        \n",
    "        # 2. 길이 확인\n",
    "        merged = merged[\n",
    "            (merged['Q'].str.len() > 1) & \n",
    "            (merged['A'].str.len() > 1)\n",
    "        ]\n",
    "        print(f\"  정제: {len(merged):,} 샘플 (길이 확인)\")\n",
    "        \n",
    "        # 3. 중복 제거\n",
    "        original_len = len(merged)\n",
    "        merged = merged.drop_duplicates(subset=['Q', 'A'], keep='first')\n",
    "        removed = original_len - len(merged)\n",
    "        print(f\"  정제: {len(merged):,} 샘플 ({removed:,} 중복 제거)\")\n",
    "        \n",
    "        # 저장\n",
    "        merged.to_csv(self.output_file, index=False, encoding='utf-8')\n",
    "        print(f\"  ✅ 저장: {self.output_file}\")\n",
    "        \n",
    "        return merged\n",
    "    \n",
    "    def print_samples(self, df):\n",
    "        \"\"\"샘플 출력\"\"\"\n",
    "        if df is None or len(df) == 0:\n",
    "            return\n",
    "        \n",
    "        print(\"\\n📊 샘플 데이터:\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        for i, row in df.head(5).iterrows():\n",
    "            print(f\"\\n[{i+1}] ({row['label']})\")\n",
    "            print(f\"  Q: {row['Q'][:60]}\")\n",
    "            \n",
    "            answer = row['A']\n",
    "            if len(answer) > 60:\n",
    "                answer = answer[:60] + \"...\"\n",
    "            \n",
    "            print(f\"  A: {answer}\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"전체 프로세스 실행\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"한국어 Q&A 코퍼스 통합 다운로더\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        dfs = []\n",
    "        \n",
    "        # 1. ChatBot 데이터 (자동)\n",
    "        df_chatbot = self.download_chatbot_data()\n",
    "        if df_chatbot is not None:\n",
    "            dfs.append(df_chatbot)\n",
    "        \n",
    "        # 2. KorQA 데이터 (수동 다운로드 후)\n",
    "        if os.path.exists('korquad_v2.json'):\n",
    "            df_korquad = self.parse_local_korquad('korquad_v2.json')\n",
    "            if df_korquad is not None:\n",
    "                dfs.append(df_korquad)\n",
    "        else:\n",
    "            print(\"\\n⚠️  KorQA 파일 없음 (선택사항)\")\n",
    "            print(\"   다운로드: https://korquad.github.io/\")\n",
    "            print(\"   압축 해제 후 korquad_v2.json을 현재 디렉토리에 배치\")\n",
    "        \n",
    "        # 병합\n",
    "        if not dfs:\n",
    "            print(\"\\n❌ 처리할 데이터가 없습니다\")\n",
    "            print(\"   최소한 ChatBot 데이터는 자동으로 다운로드됩니다\")\n",
    "            print(\"   인터넷 연결을 확인하고 다시 시도하세요\")\n",
    "            return False\n",
    "        \n",
    "        merged = self.merge_and_save(*dfs)\n",
    "        \n",
    "        if merged is None or len(merged) == 0:\n",
    "            print(\"\\n❌ 병합 실패!\")\n",
    "            return False\n",
    "        \n",
    "        # 샘플 출력\n",
    "        self.print_samples(merged)\n",
    "        \n",
    "        # 완료\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"✨ 완료!\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # 통계\n",
    "        file_size = os.path.getsize(self.output_file) / 1024 / 1024\n",
    "        \n",
    "        print(f\"\\n📈 최종 통계:\")\n",
    "        print(f\"  파일: {self.output_file}\")\n",
    "        print(f\"  샘플: {len(merged):,} 개\")\n",
    "        print(f\"  크기: {file_size:.1f}MB\")\n",
    "        print(f\"  Q 평균 길이: {merged['Q'].str.len().mean():.1f} 자\")\n",
    "        print(f\"  A 평균 길이: {merged['A'].str.len().mean():.1f} 자\")\n",
    "        \n",
    "        # 다음 단계\n",
    "        print(f\"\\n🚀 다음 단계:\")\n",
    "        print(f\"  1. merge_datasets.py 실행:\")\n",
    "        print(f\"     $ python3 merge_datasets.py\")\n",
    "        print(f\"\")\n",
    "        print(f\"  2. Config 수정:\")\n",
    "        print(f\"     DATA_PATH = 'korean_qa_data.csv'\")\n",
    "        print(f\"\")\n",
    "        print(f\"  3. 모델 재학습:\")\n",
    "        print(f\"     $ python3 korean_chatbot_complete.py\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"경고: merge_datasets.py를 사용할 때\")\n",
    "        print(\"      korean_qa_data.csv를 ChatbotData.csv와 병합할 수 있습니다\")\n",
    "        print(\"=\" * 70 + \"\\n\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 함수\"\"\"\n",
    "    downloader = KoreanQACorpusDownloader()\n",
    "    success = downloader.run()\n",
    "    \n",
    "    return 0 if success else 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        exit_code = main()\n",
    "        sys.exit(exit_code)\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n❌ 사용자에 의해 중단되었습니다\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ 예상치 못한 오류: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b56f75c-a88b-4ea7-ac86-b0b7252cd26d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4de78e-6615-4e16-a435-7435bebc8b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d087ece-bedb-4429-8a51-aea50f4f80a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4535e012-e553-4a62-b120-e37bf7abb6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
